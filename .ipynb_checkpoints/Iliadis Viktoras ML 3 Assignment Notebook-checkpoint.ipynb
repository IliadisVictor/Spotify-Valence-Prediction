{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385257b5",
   "metadata": {},
   "source": [
    "# Dissecting Spotify Valence\n",
    "In this assignment i will try to dissect Spotify's Valence metric.\n",
    "\n",
    "---\n",
    "\n",
    "> Iliadis Viktoras, Undergraduate Student <br />\n",
    "> Department of Management Science and Technology <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> iliadisviktoras@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272245ef",
   "metadata": {},
   "source": [
    "In this assignment we will try to untangle the mystery behind spotify's ( originaly Echo Nest's ) valence metric that indicates the emotional impact of a song , particulary it's ability to make you happy ( positive valence ) or sad ( negative valence) by listening to it . For cleaner code the data  etl process can be found on the [ETL.ipynb](https://github.com/IliadisVictor/Spotify-Valence-Prediction/blob/main/ETL.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a482ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sb\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c2a70",
   "metadata": {},
   "source": [
    "We are going to read the data we extracted from denodo and the Spotify API , rename the index song_id we didn't extract correctly , and set it as index . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5032ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spotify_Data= \"https://raw.githubusercontent.com/IliadisVictor/Spotify-Valence-Prediction/main/TrackData/TrackFeatures.csv\"\n",
    "Tracksdf = pd.read_csv(Spotify_Data)\n",
    "Tracksdf.columns.values[0] = \"song_id\"\n",
    "Tracksdf = Tracksdf.set_index('song_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04fa72",
   "metadata": {},
   "source": [
    "We also have to replace with dummies the numerical column time signature . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d953ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tracksdf = pd.get_dummies(Tracksdf, columns=['time_signature'], prefix='', prefix_sep='').\\\n",
    "    rename(columns={'4': 'Signature4/7', '5': 'Signature5/7', '3': 'Signature3/7' , '1': 'Signature1/7'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15525bdd",
   "metadata": {},
   "source": [
    "### Q1: Exploring which Features Influence Valence - Inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c725e4",
   "metadata": {},
   "source": [
    "Now we will run some inferential statistic methods to study how the varius audio and analysis features we extracted influence \n",
    "valence . But efore we do that let's take a look at the data we are working with .  I considered summing together all the timbre values but i lost information value and the models performed worse ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8583c814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>TotalSegments</th>\n",
       "      <th>TotalSections</th>\n",
       "      <th>AvgTatumDuration</th>\n",
       "      <th>Pitch1</th>\n",
       "      <th>Pitch2</th>\n",
       "      <th>Pitch3</th>\n",
       "      <th>Pitch4</th>\n",
       "      <th>Pitch5</th>\n",
       "      <th>Pitch6</th>\n",
       "      <th>Pitch7</th>\n",
       "      <th>Pitch8</th>\n",
       "      <th>Pitch9</th>\n",
       "      <th>Pitch10</th>\n",
       "      <th>Pitch11</th>\n",
       "      <th>Pitch12</th>\n",
       "      <th>Timbre1</th>\n",
       "      <th>Timbre2</th>\n",
       "      <th>Timbre3</th>\n",
       "      <th>Timbre4</th>\n",
       "      <th>Timbre5</th>\n",
       "      <th>Timbre6</th>\n",
       "      <th>Timbre7</th>\n",
       "      <th>Timbre8</th>\n",
       "      <th>Timbre9</th>\n",
       "      <th>Timbre10</th>\n",
       "      <th>Timbre11</th>\n",
       "      <th>Timbre12</th>\n",
       "      <th>Signature1/7</th>\n",
       "      <th>Signature3/7</th>\n",
       "      <th>Signature4/7</th>\n",
       "      <th>Signature5/7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5aAx2yezTd8zXrkmtKl66Z</th>\n",
       "      <td>0.681</td>\n",
       "      <td>0.594</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>186.054</td>\n",
       "      <td>230453</td>\n",
       "      <td>970</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.571832</td>\n",
       "      <td>0.480393</td>\n",
       "      <td>0.327399</td>\n",
       "      <td>0.227649</td>\n",
       "      <td>0.347427</td>\n",
       "      <td>0.229890</td>\n",
       "      <td>0.339872</td>\n",
       "      <td>0.402320</td>\n",
       "      <td>0.522638</td>\n",
       "      <td>0.561724</td>\n",
       "      <td>0.361254</td>\n",
       "      <td>0.297182</td>\n",
       "      <td>47.927730</td>\n",
       "      <td>3.594079</td>\n",
       "      <td>-25.474662</td>\n",
       "      <td>-8.497975</td>\n",
       "      <td>11.283421</td>\n",
       "      <td>-9.608518</td>\n",
       "      <td>2.480847</td>\n",
       "      <td>-3.206591</td>\n",
       "      <td>1.009621</td>\n",
       "      <td>6.001180</td>\n",
       "      <td>-18.966575</td>\n",
       "      <td>-1.065181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5knuzwU65gJK7IF5yJsuaW</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>101.965</td>\n",
       "      <td>251088</td>\n",
       "      <td>862</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2943</td>\n",
       "      <td>0.499654</td>\n",
       "      <td>0.298715</td>\n",
       "      <td>0.312965</td>\n",
       "      <td>0.240674</td>\n",
       "      <td>0.433773</td>\n",
       "      <td>0.349730</td>\n",
       "      <td>0.281313</td>\n",
       "      <td>0.388650</td>\n",
       "      <td>0.262365</td>\n",
       "      <td>0.431915</td>\n",
       "      <td>0.233845</td>\n",
       "      <td>0.421024</td>\n",
       "      <td>52.318747</td>\n",
       "      <td>66.142246</td>\n",
       "      <td>47.688657</td>\n",
       "      <td>13.829239</td>\n",
       "      <td>41.981918</td>\n",
       "      <td>-11.063666</td>\n",
       "      <td>-1.380892</td>\n",
       "      <td>1.131578</td>\n",
       "      <td>-8.356399</td>\n",
       "      <td>4.714077</td>\n",
       "      <td>-12.080945</td>\n",
       "      <td>0.713747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7BKLCZ1jbUBVqRi2FVlTVw</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.524</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.599</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>95.010</td>\n",
       "      <td>244960</td>\n",
       "      <td>669</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.347994</td>\n",
       "      <td>0.349380</td>\n",
       "      <td>0.176196</td>\n",
       "      <td>0.465272</td>\n",
       "      <td>0.220541</td>\n",
       "      <td>0.301170</td>\n",
       "      <td>0.145163</td>\n",
       "      <td>0.250395</td>\n",
       "      <td>0.456460</td>\n",
       "      <td>0.147130</td>\n",
       "      <td>0.394556</td>\n",
       "      <td>0.161311</td>\n",
       "      <td>49.448082</td>\n",
       "      <td>48.444740</td>\n",
       "      <td>-7.485401</td>\n",
       "      <td>-4.112776</td>\n",
       "      <td>48.958689</td>\n",
       "      <td>-12.356689</td>\n",
       "      <td>-12.970229</td>\n",
       "      <td>-4.290602</td>\n",
       "      <td>-11.967572</td>\n",
       "      <td>-1.022499</td>\n",
       "      <td>-15.331625</td>\n",
       "      <td>1.318928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3NdDpSvN911VPGivFlV5d0</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.374</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>117.973</td>\n",
       "      <td>245200</td>\n",
       "      <td>664</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2543</td>\n",
       "      <td>0.739482</td>\n",
       "      <td>0.481447</td>\n",
       "      <td>0.218806</td>\n",
       "      <td>0.176063</td>\n",
       "      <td>0.201536</td>\n",
       "      <td>0.249479</td>\n",
       "      <td>0.247473</td>\n",
       "      <td>0.275925</td>\n",
       "      <td>0.224583</td>\n",
       "      <td>0.283459</td>\n",
       "      <td>0.183745</td>\n",
       "      <td>0.258026</td>\n",
       "      <td>45.388105</td>\n",
       "      <td>12.905857</td>\n",
       "      <td>-8.822800</td>\n",
       "      <td>-14.291958</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>-8.242187</td>\n",
       "      <td>19.188872</td>\n",
       "      <td>8.583830</td>\n",
       "      <td>-1.528432</td>\n",
       "      <td>3.469480</td>\n",
       "      <td>-15.985839</td>\n",
       "      <td>-1.124039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78rIJddV4X0HkNAInEcYde</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.031</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>104.998</td>\n",
       "      <td>222041</td>\n",
       "      <td>718</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.571201</td>\n",
       "      <td>0.275758</td>\n",
       "      <td>0.361479</td>\n",
       "      <td>0.219260</td>\n",
       "      <td>0.339540</td>\n",
       "      <td>0.318723</td>\n",
       "      <td>0.294999</td>\n",
       "      <td>0.541737</td>\n",
       "      <td>0.291741</td>\n",
       "      <td>0.436024</td>\n",
       "      <td>0.255237</td>\n",
       "      <td>0.269767</td>\n",
       "      <td>51.890642</td>\n",
       "      <td>58.936022</td>\n",
       "      <td>-5.129730</td>\n",
       "      <td>-4.067110</td>\n",
       "      <td>38.045295</td>\n",
       "      <td>-30.086439</td>\n",
       "      <td>-15.993999</td>\n",
       "      <td>2.183294</td>\n",
       "      <td>-6.271883</td>\n",
       "      <td>9.430749</td>\n",
       "      <td>-11.572217</td>\n",
       "      <td>-6.324930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        danceability  energy  key  loudness  mode  \\\n",
       "song_id                                                             \n",
       "5aAx2yezTd8zXrkmtKl66Z         0.681   0.594    7    -7.028     1   \n",
       "5knuzwU65gJK7IF5yJsuaW         0.720   0.763    9    -4.068     0   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw         0.748   0.524    8    -5.599     1   \n",
       "3NdDpSvN911VPGivFlV5d0         0.735   0.451    0    -8.374     1   \n",
       "78rIJddV4X0HkNAInEcYde         0.670   0.838    0    -4.031     1   \n",
       "\n",
       "                        speechiness  acousticness  instrumentalness  liveness  \\\n",
       "song_id                                                                         \n",
       "5aAx2yezTd8zXrkmtKl66Z       0.2820        0.1650          0.000003     0.134   \n",
       "5knuzwU65gJK7IF5yJsuaW       0.0523        0.4060          0.000000     0.180   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw       0.0338        0.4140          0.000000     0.111   \n",
       "3NdDpSvN911VPGivFlV5d0       0.0585        0.0631          0.000013     0.325   \n",
       "78rIJddV4X0HkNAInEcYde       0.0362        0.0604          0.000611     0.159   \n",
       "\n",
       "                        valence    tempo  duration_ms  TotalSegments  \\\n",
       "song_id                                                                \n",
       "5aAx2yezTd8zXrkmtKl66Z   0.5350  186.054       230453            970   \n",
       "5knuzwU65gJK7IF5yJsuaW   0.7420  101.965       251088            862   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw   0.6610   95.010       244960            669   \n",
       "3NdDpSvN911VPGivFlV5d0   0.0862  117.973       245200            664   \n",
       "78rIJddV4X0HkNAInEcYde   0.7170  104.998       222041            718   \n",
       "\n",
       "                        TotalSections  AvgTatumDuration    Pitch1    Pitch2  \\\n",
       "song_id                                                                       \n",
       "5aAx2yezTd8zXrkmtKl66Z              9            0.1613  0.571832  0.480393   \n",
       "5knuzwU65gJK7IF5yJsuaW             11            0.2943  0.499654  0.298715   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw              8            0.3163  0.347994  0.349380   \n",
       "3NdDpSvN911VPGivFlV5d0              8            0.2543  0.739482  0.481447   \n",
       "78rIJddV4X0HkNAInEcYde              8            0.2857  0.571201  0.275758   \n",
       "\n",
       "                          Pitch3    Pitch4    Pitch5    Pitch6    Pitch7  \\\n",
       "song_id                                                                    \n",
       "5aAx2yezTd8zXrkmtKl66Z  0.327399  0.227649  0.347427  0.229890  0.339872   \n",
       "5knuzwU65gJK7IF5yJsuaW  0.312965  0.240674  0.433773  0.349730  0.281313   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw  0.176196  0.465272  0.220541  0.301170  0.145163   \n",
       "3NdDpSvN911VPGivFlV5d0  0.218806  0.176063  0.201536  0.249479  0.247473   \n",
       "78rIJddV4X0HkNAInEcYde  0.361479  0.219260  0.339540  0.318723  0.294999   \n",
       "\n",
       "                          Pitch8    Pitch9   Pitch10   Pitch11   Pitch12  \\\n",
       "song_id                                                                    \n",
       "5aAx2yezTd8zXrkmtKl66Z  0.402320  0.522638  0.561724  0.361254  0.297182   \n",
       "5knuzwU65gJK7IF5yJsuaW  0.388650  0.262365  0.431915  0.233845  0.421024   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw  0.250395  0.456460  0.147130  0.394556  0.161311   \n",
       "3NdDpSvN911VPGivFlV5d0  0.275925  0.224583  0.283459  0.183745  0.258026   \n",
       "78rIJddV4X0HkNAInEcYde  0.541737  0.291741  0.436024  0.255237  0.269767   \n",
       "\n",
       "                          Timbre1    Timbre2    Timbre3    Timbre4    Timbre5  \\\n",
       "song_id                                                                         \n",
       "5aAx2yezTd8zXrkmtKl66Z  47.927730   3.594079 -25.474662  -8.497975  11.283421   \n",
       "5knuzwU65gJK7IF5yJsuaW  52.318747  66.142246  47.688657  13.829239  41.981918   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw  49.448082  48.444740  -7.485401  -4.112776  48.958689   \n",
       "3NdDpSvN911VPGivFlV5d0  45.388105  12.905857  -8.822800 -14.291958   0.926185   \n",
       "78rIJddV4X0HkNAInEcYde  51.890642  58.936022  -5.129730  -4.067110  38.045295   \n",
       "\n",
       "                          Timbre6    Timbre7   Timbre8    Timbre9  Timbre10  \\\n",
       "song_id                                                                       \n",
       "5aAx2yezTd8zXrkmtKl66Z  -9.608518   2.480847 -3.206591   1.009621  6.001180   \n",
       "5knuzwU65gJK7IF5yJsuaW -11.063666  -1.380892  1.131578  -8.356399  4.714077   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw -12.356689 -12.970229 -4.290602 -11.967572 -1.022499   \n",
       "3NdDpSvN911VPGivFlV5d0  -8.242187  19.188872  8.583830  -1.528432  3.469480   \n",
       "78rIJddV4X0HkNAInEcYde -30.086439 -15.993999  2.183294  -6.271883  9.430749   \n",
       "\n",
       "                         Timbre11  Timbre12  Signature1/7  Signature3/7  \\\n",
       "song_id                                                                   \n",
       "5aAx2yezTd8zXrkmtKl66Z -18.966575 -1.065181             0             0   \n",
       "5knuzwU65gJK7IF5yJsuaW -12.080945  0.713747             0             0   \n",
       "7BKLCZ1jbUBVqRi2FVlTVw -15.331625  1.318928             0             0   \n",
       "3NdDpSvN911VPGivFlV5d0 -15.985839 -1.124039             0             0   \n",
       "78rIJddV4X0HkNAInEcYde -11.572217 -6.324930             0             0   \n",
       "\n",
       "                        Signature4/7  Signature5/7  \n",
       "song_id                                             \n",
       "5aAx2yezTd8zXrkmtKl66Z             1             0  \n",
       "5knuzwU65gJK7IF5yJsuaW             1             0  \n",
       "7BKLCZ1jbUBVqRi2FVlTVw             1             0  \n",
       "3NdDpSvN911VPGivFlV5d0             1             0  \n",
       "78rIJddV4X0HkNAInEcYde             1             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "Tracksdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c1afa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>TotalSegments</th>\n",
       "      <th>TotalSections</th>\n",
       "      <th>AvgTatumDuration</th>\n",
       "      <th>Pitch1</th>\n",
       "      <th>Pitch2</th>\n",
       "      <th>Pitch3</th>\n",
       "      <th>Pitch4</th>\n",
       "      <th>Pitch5</th>\n",
       "      <th>Pitch6</th>\n",
       "      <th>Pitch7</th>\n",
       "      <th>Pitch8</th>\n",
       "      <th>Pitch9</th>\n",
       "      <th>Pitch10</th>\n",
       "      <th>Pitch11</th>\n",
       "      <th>Pitch12</th>\n",
       "      <th>Timbre1</th>\n",
       "      <th>Timbre2</th>\n",
       "      <th>Timbre3</th>\n",
       "      <th>Timbre4</th>\n",
       "      <th>Timbre5</th>\n",
       "      <th>Timbre6</th>\n",
       "      <th>Timbre7</th>\n",
       "      <th>Timbre8</th>\n",
       "      <th>Timbre9</th>\n",
       "      <th>Timbre10</th>\n",
       "      <th>Timbre11</th>\n",
       "      <th>Timbre12</th>\n",
       "      <th>Signature1/7</th>\n",
       "      <th>Signature3/7</th>\n",
       "      <th>Signature4/7</th>\n",
       "      <th>Signature5/7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>1.388000e+04</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "      <td>13880.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.687501</td>\n",
       "      <td>0.665705</td>\n",
       "      <td>5.353746</td>\n",
       "      <td>-6.299205</td>\n",
       "      <td>0.528746</td>\n",
       "      <td>0.153765</td>\n",
       "      <td>0.248970</td>\n",
       "      <td>0.007918</td>\n",
       "      <td>0.182294</td>\n",
       "      <td>0.503584</td>\n",
       "      <td>122.342550</td>\n",
       "      <td>2.072196e+05</td>\n",
       "      <td>690.722262</td>\n",
       "      <td>7.146398</td>\n",
       "      <td>0.258695</td>\n",
       "      <td>0.478827</td>\n",
       "      <td>0.495655</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>0.324971</td>\n",
       "      <td>0.319240</td>\n",
       "      <td>0.317516</td>\n",
       "      <td>0.319270</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.309739</td>\n",
       "      <td>0.286545</td>\n",
       "      <td>0.307735</td>\n",
       "      <td>48.283386</td>\n",
       "      <td>28.610597</td>\n",
       "      <td>-6.376996</td>\n",
       "      <td>-2.713711</td>\n",
       "      <td>30.821968</td>\n",
       "      <td>-13.279664</td>\n",
       "      <td>-1.971258</td>\n",
       "      <td>-1.274957</td>\n",
       "      <td>-8.975840</td>\n",
       "      <td>3.102333</td>\n",
       "      <td>-14.706334</td>\n",
       "      <td>-1.025597</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.032709</td>\n",
       "      <td>0.940274</td>\n",
       "      <td>0.023199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.141401</td>\n",
       "      <td>0.161338</td>\n",
       "      <td>3.636216</td>\n",
       "      <td>2.395854</td>\n",
       "      <td>0.499191</td>\n",
       "      <td>0.130939</td>\n",
       "      <td>0.232287</td>\n",
       "      <td>0.060413</td>\n",
       "      <td>0.153279</td>\n",
       "      <td>0.214519</td>\n",
       "      <td>28.106891</td>\n",
       "      <td>5.011249e+04</td>\n",
       "      <td>209.672281</td>\n",
       "      <td>2.349873</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>0.127440</td>\n",
       "      <td>0.127420</td>\n",
       "      <td>0.089318</td>\n",
       "      <td>0.088517</td>\n",
       "      <td>0.095494</td>\n",
       "      <td>0.095237</td>\n",
       "      <td>0.096621</td>\n",
       "      <td>0.098606</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.097609</td>\n",
       "      <td>0.090889</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>3.164404</td>\n",
       "      <td>29.607600</td>\n",
       "      <td>23.113681</td>\n",
       "      <td>13.497310</td>\n",
       "      <td>14.076035</td>\n",
       "      <td>8.700097</td>\n",
       "      <td>10.069160</td>\n",
       "      <td>5.925268</td>\n",
       "      <td>7.072723</td>\n",
       "      <td>4.826244</td>\n",
       "      <td>5.428477</td>\n",
       "      <td>5.149677</td>\n",
       "      <td>0.061678</td>\n",
       "      <td>0.177880</td>\n",
       "      <td>0.236987</td>\n",
       "      <td>0.150540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-34.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>46.489000</td>\n",
       "      <td>3.040000e+04</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.082490</td>\n",
       "      <td>0.083986</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.057868</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>0.057301</td>\n",
       "      <td>0.058309</td>\n",
       "      <td>0.047793</td>\n",
       "      <td>0.054814</td>\n",
       "      <td>0.046969</td>\n",
       "      <td>0.031498</td>\n",
       "      <td>19.942672</td>\n",
       "      <td>-180.360200</td>\n",
       "      <td>-130.563273</td>\n",
       "      <td>-62.800714</td>\n",
       "      <td>-31.247225</td>\n",
       "      <td>-50.733129</td>\n",
       "      <td>-48.313851</td>\n",
       "      <td>-38.974630</td>\n",
       "      <td>-46.796992</td>\n",
       "      <td>-21.627510</td>\n",
       "      <td>-42.410611</td>\n",
       "      <td>-22.119371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.474250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>99.951000</td>\n",
       "      <td>1.776685e+05</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.393586</td>\n",
       "      <td>0.409838</td>\n",
       "      <td>0.273848</td>\n",
       "      <td>0.246563</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.250315</td>\n",
       "      <td>0.248166</td>\n",
       "      <td>0.248617</td>\n",
       "      <td>0.238296</td>\n",
       "      <td>0.239658</td>\n",
       "      <td>0.220594</td>\n",
       "      <td>0.238231</td>\n",
       "      <td>46.548324</td>\n",
       "      <td>10.679291</td>\n",
       "      <td>-20.943574</td>\n",
       "      <td>-11.722833</td>\n",
       "      <td>21.482720</td>\n",
       "      <td>-18.997647</td>\n",
       "      <td>-8.819108</td>\n",
       "      <td>-4.618286</td>\n",
       "      <td>-13.629276</td>\n",
       "      <td>-0.101539</td>\n",
       "      <td>-18.198586</td>\n",
       "      <td>-4.353169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-6.016500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>121.940000</td>\n",
       "      <td>2.012645e+05</td>\n",
       "      <td>674.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.471677</td>\n",
       "      <td>0.492805</td>\n",
       "      <td>0.333674</td>\n",
       "      <td>0.302257</td>\n",
       "      <td>0.316833</td>\n",
       "      <td>0.310505</td>\n",
       "      <td>0.309207</td>\n",
       "      <td>0.312024</td>\n",
       "      <td>0.299696</td>\n",
       "      <td>0.301608</td>\n",
       "      <td>0.279433</td>\n",
       "      <td>0.299745</td>\n",
       "      <td>48.525641</td>\n",
       "      <td>29.383468</td>\n",
       "      <td>-5.323188</td>\n",
       "      <td>-3.306054</td>\n",
       "      <td>30.568650</td>\n",
       "      <td>-13.915347</td>\n",
       "      <td>-2.007040</td>\n",
       "      <td>-0.766320</td>\n",
       "      <td>-8.820139</td>\n",
       "      <td>3.153664</td>\n",
       "      <td>-14.317859</td>\n",
       "      <td>-1.043187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.751000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>140.057250</td>\n",
       "      <td>2.286135e+05</td>\n",
       "      <td>791.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.557758</td>\n",
       "      <td>0.579977</td>\n",
       "      <td>0.397164</td>\n",
       "      <td>0.366306</td>\n",
       "      <td>0.386277</td>\n",
       "      <td>0.379796</td>\n",
       "      <td>0.380393</td>\n",
       "      <td>0.382808</td>\n",
       "      <td>0.372463</td>\n",
       "      <td>0.372994</td>\n",
       "      <td>0.344766</td>\n",
       "      <td>0.370655</td>\n",
       "      <td>50.363340</td>\n",
       "      <td>48.111860</td>\n",
       "      <td>9.103728</td>\n",
       "      <td>5.518085</td>\n",
       "      <td>40.031760</td>\n",
       "      <td>-8.236027</td>\n",
       "      <td>4.631543</td>\n",
       "      <td>2.664409</td>\n",
       "      <td>-4.227990</td>\n",
       "      <td>6.245511</td>\n",
       "      <td>-10.718956</td>\n",
       "      <td>2.342895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.107000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>212.117000</td>\n",
       "      <td>1.109080e+06</td>\n",
       "      <td>4432.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.959913</td>\n",
       "      <td>0.945366</td>\n",
       "      <td>0.829350</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.729302</td>\n",
       "      <td>0.783105</td>\n",
       "      <td>0.823321</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.739642</td>\n",
       "      <td>0.782761</td>\n",
       "      <td>0.686903</td>\n",
       "      <td>0.760042</td>\n",
       "      <td>58.709161</td>\n",
       "      <td>187.759321</td>\n",
       "      <td>162.842875</td>\n",
       "      <td>91.359233</td>\n",
       "      <td>96.155911</td>\n",
       "      <td>69.273006</td>\n",
       "      <td>41.779033</td>\n",
       "      <td>25.081640</td>\n",
       "      <td>17.196300</td>\n",
       "      <td>32.357779</td>\n",
       "      <td>3.585169</td>\n",
       "      <td>31.887818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       danceability        energy           key      loudness          mode  \\\n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean       0.687501      0.665705      5.353746     -6.299205      0.528746   \n",
       "std        0.141401      0.161338      3.636216      2.395854      0.499191   \n",
       "min        0.118000      0.005430      0.000000    -34.475000      0.000000   \n",
       "25%        0.598000      0.566000      2.000000     -7.474250      0.000000   \n",
       "50%        0.703000      0.680000      6.000000     -6.016500      1.000000   \n",
       "75%        0.792000      0.784000      8.000000     -4.751000      1.000000   \n",
       "max        0.980000      0.999000     11.000000      1.107000      1.000000   \n",
       "\n",
       "        speechiness  acousticness  instrumentalness      liveness  \\\n",
       "count  13880.000000  13880.000000      13880.000000  13880.000000   \n",
       "mean       0.153765      0.248970          0.007918      0.182294   \n",
       "std        0.130939      0.232287          0.060413      0.153279   \n",
       "min        0.023100      0.000003          0.000000      0.019700   \n",
       "25%        0.050100      0.058800          0.000000      0.096000   \n",
       "50%        0.098300      0.177000          0.000000      0.122000   \n",
       "75%        0.240000      0.380000          0.000014      0.210000   \n",
       "max        0.966000      0.994000          0.960000      0.990000   \n",
       "\n",
       "            valence         tempo   duration_ms  TotalSegments  TotalSections  \\\n",
       "count  13880.000000  13880.000000  1.388000e+04   13880.000000   13880.000000   \n",
       "mean       0.503584    122.342550  2.072196e+05     690.722262       7.146398   \n",
       "std        0.214519     28.106891  5.011249e+04     209.672281       2.349873   \n",
       "min        0.035400     46.489000  3.040000e+04      40.000000       1.000000   \n",
       "25%        0.341000     99.951000  1.776685e+05     562.000000       5.000000   \n",
       "50%        0.502000    121.940000  2.012645e+05     674.000000       7.000000   \n",
       "75%        0.668000    140.057250  2.286135e+05     791.000000       9.000000   \n",
       "max        0.982000    212.117000  1.109080e+06    4432.000000      27.000000   \n",
       "\n",
       "       AvgTatumDuration        Pitch1        Pitch2        Pitch3  \\\n",
       "count      13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean           0.258695      0.478827      0.495655      0.338728   \n",
       "std            0.060163      0.127440      0.127420      0.089318   \n",
       "min            0.141500      0.072000      0.082490      0.083986   \n",
       "25%            0.214200      0.393586      0.409838      0.273848   \n",
       "50%            0.248000      0.471677      0.492805      0.333674   \n",
       "75%            0.300100      0.557758      0.579977      0.397164   \n",
       "max            0.639300      0.959913      0.945366      0.829350   \n",
       "\n",
       "             Pitch4        Pitch5        Pitch6        Pitch7        Pitch8  \\\n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean       0.311019      0.324971      0.319240      0.317516      0.319270   \n",
       "std        0.088517      0.095494      0.095237      0.096621      0.098606   \n",
       "min        0.062971      0.057868      0.059089      0.057301      0.058309   \n",
       "25%        0.246563      0.255990      0.250315      0.248166      0.248617   \n",
       "50%        0.302257      0.316833      0.310505      0.309207      0.312024   \n",
       "75%        0.366306      0.386277      0.379796      0.380393      0.382808   \n",
       "max        0.689799      0.729302      0.783105      0.823321      0.771689   \n",
       "\n",
       "             Pitch9       Pitch10       Pitch11       Pitch12       Timbre1  \\\n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean       0.309600      0.309739      0.286545      0.307735     48.283386   \n",
       "std        0.097859      0.097609      0.090889      0.098361      3.164404   \n",
       "min        0.047793      0.054814      0.046969      0.031498     19.942672   \n",
       "25%        0.238296      0.239658      0.220594      0.238231     46.548324   \n",
       "50%        0.299696      0.301608      0.279433      0.299745     48.525641   \n",
       "75%        0.372463      0.372994      0.344766      0.370655     50.363340   \n",
       "max        0.739642      0.782761      0.686903      0.760042     58.709161   \n",
       "\n",
       "            Timbre2       Timbre3       Timbre4       Timbre5       Timbre6  \\\n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean      28.610597     -6.376996     -2.713711     30.821968    -13.279664   \n",
       "std       29.607600     23.113681     13.497310     14.076035      8.700097   \n",
       "min     -180.360200   -130.563273    -62.800714    -31.247225    -50.733129   \n",
       "25%       10.679291    -20.943574    -11.722833     21.482720    -18.997647   \n",
       "50%       29.383468     -5.323188     -3.306054     30.568650    -13.915347   \n",
       "75%       48.111860      9.103728      5.518085     40.031760     -8.236027   \n",
       "max      187.759321    162.842875     91.359233     96.155911     69.273006   \n",
       "\n",
       "            Timbre7       Timbre8       Timbre9      Timbre10      Timbre11  \\\n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000   \n",
       "mean      -1.971258     -1.274957     -8.975840      3.102333    -14.706334   \n",
       "std       10.069160      5.925268      7.072723      4.826244      5.428477   \n",
       "min      -48.313851    -38.974630    -46.796992    -21.627510    -42.410611   \n",
       "25%       -8.819108     -4.618286    -13.629276     -0.101539    -18.198586   \n",
       "50%       -2.007040     -0.766320     -8.820139      3.153664    -14.317859   \n",
       "75%        4.631543      2.664409     -4.227990      6.245511    -10.718956   \n",
       "max       41.779033     25.081640     17.196300     32.357779      3.585169   \n",
       "\n",
       "           Timbre12  Signature1/7  Signature3/7  Signature4/7  Signature5/7  \n",
       "count  13880.000000  13880.000000  13880.000000  13880.000000  13880.000000  \n",
       "mean      -1.025597      0.003818      0.032709      0.940274      0.023199  \n",
       "std        5.149677      0.061678      0.177880      0.236987      0.150540  \n",
       "min      -22.119371      0.000000      0.000000      0.000000      0.000000  \n",
       "25%       -4.353169      0.000000      0.000000      1.000000      0.000000  \n",
       "50%       -1.043187      0.000000      0.000000      1.000000      0.000000  \n",
       "75%        2.342895      0.000000      0.000000      1.000000      0.000000  \n",
       "max       31.887818      1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tracksdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ecc30",
   "metadata": {},
   "source": [
    "####  Pearson Correlation \n",
    "We start with the Pearson method , the results where not very encouraging as far as regression is concerned the two most highly correlated features with valence\n",
    "where the energy with 0.36  and the value of Timbre 4 0.356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17abb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_list = ['Timbre1','Timbre2' , 'Timbre3', 'Timbre4', 'Timbre5', 'Timbre6', 'Timbre7', 'Timbre8', 'Timbre9', 'Timbre10', 'Timbre11', 'Timbre12']\n",
    "# Tracksdf['Timbre'] = Tracksdf[columns_list].sum(axis=1)\n",
    "\n",
    "# Tracksdf.drop(['Timbre1','Timbre2' , 'Timbre3', 'Timbre4', 'Timbre5', 'Timbre6', 'Timbre7', 'Timbre8', 'Timbre9', 'Timbre10', 'Timbre11', 'Timbre12'], axis = 1,inplace=True)\n",
    "pearsoncorr = Tracksdf.corr(method='pearson')\n",
    "PearsonColumns = ['danceability','energy','loudness','Pitch7','Pitch10','Pitch11','Pitch12','Timbre1','Timbre2','Timbre7','Timbre10',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745f8857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>TotalSegments</th>\n",
       "      <th>TotalSections</th>\n",
       "      <th>AvgTatumDuration</th>\n",
       "      <th>Pitch1</th>\n",
       "      <th>Pitch2</th>\n",
       "      <th>Pitch3</th>\n",
       "      <th>Pitch4</th>\n",
       "      <th>Pitch5</th>\n",
       "      <th>Pitch6</th>\n",
       "      <th>Pitch7</th>\n",
       "      <th>Pitch8</th>\n",
       "      <th>Pitch9</th>\n",
       "      <th>Pitch10</th>\n",
       "      <th>Pitch11</th>\n",
       "      <th>Pitch12</th>\n",
       "      <th>Timbre1</th>\n",
       "      <th>Timbre2</th>\n",
       "      <th>Timbre3</th>\n",
       "      <th>Timbre4</th>\n",
       "      <th>Timbre5</th>\n",
       "      <th>Timbre6</th>\n",
       "      <th>Timbre7</th>\n",
       "      <th>Timbre8</th>\n",
       "      <th>Timbre9</th>\n",
       "      <th>Timbre10</th>\n",
       "      <th>Timbre11</th>\n",
       "      <th>Timbre12</th>\n",
       "      <th>Signature1/7</th>\n",
       "      <th>Signature3/7</th>\n",
       "      <th>Signature4/7</th>\n",
       "      <th>Signature5/7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.239886</td>\n",
       "      <td>0.367636</td>\n",
       "      <td>0.043517</td>\n",
       "      <td>0.269421</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>-0.02502</td>\n",
       "      <td>-0.026301</td>\n",
       "      <td>-0.073128</td>\n",
       "      <td>0.079136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050688</td>\n",
       "      <td>-0.117293</td>\n",
       "      <td>0.091846</td>\n",
       "      <td>-0.049721</td>\n",
       "      <td>-0.064276</td>\n",
       "      <td>-0.084372</td>\n",
       "      <td>-0.092841</td>\n",
       "      <td>0.064952</td>\n",
       "      <td>0.08029</td>\n",
       "      <td>0.103487</td>\n",
       "      <td>0.147716</td>\n",
       "      <td>0.230743</td>\n",
       "      <td>0.18511</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>0.205398</td>\n",
       "      <td>0.267204</td>\n",
       "      <td>0.330242</td>\n",
       "      <td>0.292236</td>\n",
       "      <td>0.253229</td>\n",
       "      <td>0.171616</td>\n",
       "      <td>0.356232</td>\n",
       "      <td>0.086417</td>\n",
       "      <td>0.067297</td>\n",
       "      <td>-0.232641</td>\n",
       "      <td>0.085429</td>\n",
       "      <td>-0.12235</td>\n",
       "      <td>0.205754</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.065792</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>-0.070196</td>\n",
       "      <td>0.077428</td>\n",
       "      <td>-0.023849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         danceability    energy       key  loudness      mode  speechiness  \\\n",
       "valence      0.239886  0.367636  0.043517  0.269421 -0.035945     -0.02502   \n",
       "\n",
       "         acousticness  instrumentalness  liveness  valence     tempo  \\\n",
       "valence     -0.026301         -0.073128  0.079136      1.0  0.050688   \n",
       "\n",
       "         duration_ms  TotalSegments  TotalSections  AvgTatumDuration  \\\n",
       "valence    -0.117293       0.091846      -0.049721         -0.064276   \n",
       "\n",
       "           Pitch1    Pitch2    Pitch3   Pitch4    Pitch5    Pitch6    Pitch7  \\\n",
       "valence -0.084372 -0.092841  0.064952  0.08029  0.103487  0.147716  0.230743   \n",
       "\n",
       "          Pitch8    Pitch9   Pitch10   Pitch11   Pitch12   Timbre1   Timbre2  \\\n",
       "valence  0.18511  0.280791  0.205398  0.267204  0.330242  0.292236  0.253229   \n",
       "\n",
       "          Timbre3   Timbre4   Timbre5   Timbre6   Timbre7   Timbre8  Timbre9  \\\n",
       "valence  0.171616  0.356232  0.086417  0.067297 -0.232641  0.085429 -0.12235   \n",
       "\n",
       "         Timbre10  Timbre11  Timbre12  Signature1/7  Signature3/7  \\\n",
       "valence  0.205754  0.040755  0.065792     -0.036849     -0.070196   \n",
       "\n",
       "         Signature4/7  Signature5/7  \n",
       "valence      0.077428     -0.023849  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsoncorr.loc[['valence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa90071",
   "metadata": {},
   "source": [
    "From the pearson correlation method we find that the important features are the ones below, we didn't find a strong correlation anywhere so  we show features with a co-efficient 0.2 and greater or -0.2 and smaller. \n",
    "* Danceability 0.24\n",
    "* Energy 0.36\n",
    "* Loudness 0.26\n",
    "* Pitch 7 0.23\n",
    "* Pitch 9 0.28\n",
    "* Pitch 10 0.2\n",
    "* Pitch 11 0.26\n",
    "* Pitch 12 0.33\n",
    "* Timbre 1 0.29\n",
    "* Timbre 2 0.25\n",
    "* Timbre 4 0.35\n",
    "* Timbre 7  -0.23\n",
    "* Timbre 10 0.2\n",
    "* Summed Timbre 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d1851",
   "metadata": {},
   "source": [
    "####  Spearman's method \n",
    "The differences are negligeble and lead us to the same features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60af2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>TotalSegments</th>\n",
       "      <th>TotalSections</th>\n",
       "      <th>AvgTatumDuration</th>\n",
       "      <th>Pitch1</th>\n",
       "      <th>Pitch2</th>\n",
       "      <th>Pitch3</th>\n",
       "      <th>Pitch4</th>\n",
       "      <th>Pitch5</th>\n",
       "      <th>Pitch6</th>\n",
       "      <th>Pitch7</th>\n",
       "      <th>Pitch8</th>\n",
       "      <th>Pitch9</th>\n",
       "      <th>Pitch10</th>\n",
       "      <th>Pitch11</th>\n",
       "      <th>Pitch12</th>\n",
       "      <th>Timbre1</th>\n",
       "      <th>Timbre2</th>\n",
       "      <th>Timbre3</th>\n",
       "      <th>Timbre4</th>\n",
       "      <th>Timbre5</th>\n",
       "      <th>Timbre6</th>\n",
       "      <th>Timbre7</th>\n",
       "      <th>Timbre8</th>\n",
       "      <th>Timbre9</th>\n",
       "      <th>Timbre10</th>\n",
       "      <th>Timbre11</th>\n",
       "      <th>Timbre12</th>\n",
       "      <th>Signature1/7</th>\n",
       "      <th>Signature3/7</th>\n",
       "      <th>Signature4/7</th>\n",
       "      <th>Signature5/7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.219268</td>\n",
       "      <td>0.364085</td>\n",
       "      <td>0.039141</td>\n",
       "      <td>0.275062</td>\n",
       "      <td>-0.033783</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>-0.120471</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>-0.130841</td>\n",
       "      <td>0.105524</td>\n",
       "      <td>-0.048125</td>\n",
       "      <td>-0.046291</td>\n",
       "      <td>-0.061099</td>\n",
       "      <td>-0.09151</td>\n",
       "      <td>0.054641</td>\n",
       "      <td>0.078232</td>\n",
       "      <td>0.111782</td>\n",
       "      <td>0.155103</td>\n",
       "      <td>0.234569</td>\n",
       "      <td>0.193096</td>\n",
       "      <td>0.288806</td>\n",
       "      <td>0.210511</td>\n",
       "      <td>0.272076</td>\n",
       "      <td>0.330364</td>\n",
       "      <td>0.287292</td>\n",
       "      <td>0.234007</td>\n",
       "      <td>0.167511</td>\n",
       "      <td>0.343974</td>\n",
       "      <td>0.08187</td>\n",
       "      <td>0.059031</td>\n",
       "      <td>-0.229339</td>\n",
       "      <td>0.088061</td>\n",
       "      <td>-0.123909</td>\n",
       "      <td>0.209505</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>0.058284</td>\n",
       "      <td>-0.035777</td>\n",
       "      <td>-0.069924</td>\n",
       "      <td>0.076665</td>\n",
       "      <td>-0.023409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         danceability    energy       key  loudness      mode  speechiness  \\\n",
       "valence      0.219268  0.364085  0.039141  0.275062 -0.033783     0.008527   \n",
       "\n",
       "         acousticness  instrumentalness  liveness  valence     tempo  \\\n",
       "valence      0.021382         -0.120471  0.005834      1.0  0.042433   \n",
       "\n",
       "         duration_ms  TotalSegments  TotalSections  AvgTatumDuration  \\\n",
       "valence    -0.130841       0.105524      -0.048125         -0.046291   \n",
       "\n",
       "           Pitch1   Pitch2    Pitch3    Pitch4    Pitch5    Pitch6    Pitch7  \\\n",
       "valence -0.061099 -0.09151  0.054641  0.078232  0.111782  0.155103  0.234569   \n",
       "\n",
       "           Pitch8    Pitch9   Pitch10   Pitch11   Pitch12   Timbre1   Timbre2  \\\n",
       "valence  0.193096  0.288806  0.210511  0.272076  0.330364  0.287292  0.234007   \n",
       "\n",
       "          Timbre3   Timbre4  Timbre5   Timbre6   Timbre7   Timbre8   Timbre9  \\\n",
       "valence  0.167511  0.343974  0.08187  0.059031 -0.229339  0.088061 -0.123909   \n",
       "\n",
       "         Timbre10  Timbre11  Timbre12  Signature1/7  Signature3/7  \\\n",
       "valence  0.209505  0.038511  0.058284     -0.035777     -0.069924   \n",
       "\n",
       "         Signature4/7  Signature5/7  \n",
       "valence      0.076665     -0.023409  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmansncorr = Tracksdf.corr(method='spearman')\n",
    "spearmansncorr.loc[['valence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e491055",
   "metadata": {},
   "source": [
    "#### Filter Methods Synopsis . \n",
    "One way of selecting features would be by the \"filter\" method , setting a limit like we did above , and keeping by the Pearson or The Spearman's correlation the ones that have a value above our limit . But we are going to also try regression and keep the correlation's we calculated (Pearson and Spearmans) to remove highly correlated columns beside the target column ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56596946",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "Before we continue we must check for highly correlated features in our data , because in truth they provide the same information so if we find features except our target valence feature , with correlation greater than 0.8 we remove one of them.\n",
    "We use the **Pearson correlation** here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0278d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AvgTatumDuration', 'Timbre1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = Tracksdf.drop('valence', axis=1).corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9f69c",
   "metadata": {},
   "source": [
    "So we found that the avg tatum duration and the Timbre 1 value are highly correlated , so we remove from our data \n",
    "the one that is less correlated with our target value valence which is AvgTatumDuration . This is probably not going to any difference\n",
    "because both columns ( Tatum , and Timbre1) have close to zero correlation with our target . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52505eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tracksdf.drop('AvgTatumDuration', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b938418",
   "metadata": {},
   "source": [
    "**Running multiple linear regression , showed many features were redundant in terms of predictive capability and the condition number indicated strong multicollinearity . So we are have a subset selection problem , and we are going to deal with it with:**\n",
    "* forward stepwise selection : picking the best model for 1 to p variables , where p = total independent variables .\n",
    "* backward stepwise selection : picking the best model for p variables , and backtracking 1 variable each step . \n",
    "Judging based on adjusted R^2\n",
    "\n",
    "Let's start with the forward stepwise selection, \n",
    "For both the forward and backward , we have the process_subset that takes the current set of columns chosen at the current iteration , and fits the model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6013c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(y, data, feature_set):\n",
    "    X = data.loc[:, feature_set].values\n",
    "    X = sm.add_constant(X)\n",
    "    names = ['intercept']\n",
    "    names.extend(feature_set)\n",
    "    model = sm.OLS(y, X)\n",
    "    model.data.xnames = names\n",
    "    regr = model.fit()\n",
    "    return regr\n",
    "\n",
    "def forward_add_variable(data, exog, selected, to_select):\n",
    "    best_rsquared = 0\n",
    "    best_model = None\n",
    "    best_column = None\n",
    "    y = data.loc[:, exog]\n",
    "    \n",
    "    for column in to_select:\n",
    "        new_selected = selected + [column]\n",
    "        regr = process_subset(y, data, new_selected)\n",
    "        if regr.rsquared > best_rsquared:\n",
    "            best_rsquared = regr.rsquared\n",
    "            best_model = regr\n",
    "            best_column = column\n",
    "    \n",
    "    return best_model, best_column\n",
    "def forward_stepwise_selection(data, exog):\n",
    "\n",
    "    best_models = []\n",
    "    best_model = None\n",
    "    selected = []\n",
    "    to_select = [ x for x in data.columns if x != exog ]\n",
    "\n",
    "    p = len(to_select) + 1\n",
    "\n",
    "    for i in range(1, p):\n",
    "#         print(f'Finding the best model for {i} variable{\"s\" if i > 1 else \"\"}')\n",
    "        model, best_column = forward_add_variable(data, exog, selected, to_select)\n",
    "        selected.append(best_column)\n",
    "        to_select.remove(best_column)\n",
    "        if not best_model or model.rsquared_adj > best_model.rsquared_adj:\n",
    "            best_model = model\n",
    "#         print(selected)\n",
    "        best_models.append(model)\n",
    "        \n",
    "#     print(f'Fitted {1 + p*(p+1)//2} models')\n",
    "    return best_model, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe4e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall model: 37 ['intercept', 'energy', 'Timbre7', 'Timbre4', 'Pitch12', 'Pitch2', 'danceability', 'Pitch9', 'Timbre1', 'loudness', 'Pitch6', 'Pitch3', 'Pitch5', 'Pitch1', 'duration_ms', 'TotalSegments', 'speechiness', 'Timbre6', 'Timbre11', 'tempo', 'acousticness', 'Timbre5', 'liveness', 'Timbre3', 'Timbre10', 'mode', 'TotalSections', 'Pitch8', 'Timbre8', 'Pitch10', 'key', 'Pitch4', 'Timbre9', 'Timbre2', 'Timbre12', 'instrumentalness', 'Pitch11']\n"
     ]
    }
   ],
   "source": [
    "best_model, _ = forward_stepwise_selection(Tracksdf, 'valence')\n",
    "print('Best overall model:', len(best_model.model.exog_names), best_model.model.exog_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aebdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_remove_variable(data, exog, selected):\n",
    "    \n",
    "    best_rsquared = 0\n",
    "    best_model = None\n",
    "    best_column = None\n",
    "    y = data.loc[:, exog]\n",
    "    \n",
    "    for column in selected:\n",
    "        new_selected = selected[:]\n",
    "        new_selected.remove(column)\n",
    "        regr = process_subset(y, data, new_selected)\n",
    "        if regr.rsquared > best_rsquared:\n",
    "            best_rsquared = regr.rsquared\n",
    "            best_model = regr\n",
    "            best_column = column\n",
    "    \n",
    "    return best_model, best_column\n",
    "\n",
    "def backward_stepwise_selection(data, exog):\n",
    "\n",
    "    best_models = []\n",
    "    selected = [ x for x in data.columns if x != exog ]\n",
    "\n",
    "    p = len(selected) + 1\n",
    "\n",
    "#     print(f'Finding the best model for {p - 1} variables')\n",
    "#     print(selected)\n",
    "    y = data.loc[:, exog]\n",
    "    best_model = process_subset(y, data, selected)\n",
    "    best_models.append(best_model)\n",
    "\n",
    "    for i in reversed(range(2, p)):\n",
    "#         print(f'Finding the best model for {i - 1} variable{\"s\" if (i - 1) > 1 else \"\"}')\n",
    "        model, best_column = backward_remove_variable(data, exog, selected)\n",
    "        selected.remove(best_column)\n",
    "        if not best_model or model.rsquared_adj > best_model.rsquared_adj:\n",
    "            best_model = model\n",
    "#         print(selected)\n",
    "        best_models.append(model)\n",
    "        \n",
    "#     print(f'Fitted {1 + p*(p+1)//2} models')\n",
    "    return best_model, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2e1f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall model: 37 ['intercept', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'tempo', 'duration_ms', 'TotalSegments', 'TotalSections', 'Pitch1', 'Pitch2', 'Pitch3', 'Pitch4', 'Pitch5', 'Pitch6', 'Pitch8', 'Pitch9', 'Pitch10', 'Pitch11', 'Pitch12', 'Timbre1', 'Timbre2', 'Timbre3', 'Timbre4', 'Timbre5', 'Timbre6', 'Timbre7', 'Timbre8', 'Timbre9', 'Timbre10', 'Timbre11', 'Timbre12']\n"
     ]
    }
   ],
   "source": [
    "best_model,bestmodels= backward_stepwise_selection(Tracksdf, 'valence')\n",
    "print('Best overall model:', len(best_model.model.exog_names), best_model.model.exog_names)\n",
    "RegressionBest = best_model.model.exog_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9b3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_selection(models, best_model, reverse=True):\n",
    "    if reverse:\n",
    "        models = models[::-1]\n",
    "    all_rsquared = np.array([ x.rsquared  for x in models ])\n",
    "    all_rsquared_adj = np.array([ x.rsquared_adj  for x in models ])\n",
    "    best_indx =len(best_model.model.exog_names)\n",
    "    print(best_indx)\n",
    "    x = np.arange(1, len(all_rsquared)+1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, all_rsquared, marker='*', label='$R^2$')\n",
    "    plt.plot(x, all_rsquared_adj, marker='o', label='Adjusted $R^2$')\n",
    "    plt.plot(best_indx, all_rsquared_adj[best_indx], marker='x', markersize=14, color='k')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553f8da",
   "metadata": {},
   "source": [
    "Let's see how  the Adjusted  R<sup>2</sup> evolves , since the 2 methods return the same columns we plot once . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab4feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_selection(bestmodels, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e631f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>valence</td>     <th>  R-squared:         </th>  <td>   0.484</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.483</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   360.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 24 Feb 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:10:38</td>     <th>  Log-Likelihood:    </th>  <td>  6263.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13880</td>      <th>  AIC:               </th> <td>-1.245e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13843</td>      <th>  BIC:               </th> <td>-1.217e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    36</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>        <td>   -1.2210</td> <td>    0.077</td> <td>  -15.791</td> <td> 0.000</td> <td>   -1.373</td> <td>   -1.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>    0.2055</td> <td>    0.016</td> <td>   12.755</td> <td> 0.000</td> <td>    0.174</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>           <td>   -0.0239</td> <td>    0.021</td> <td>   -1.133</td> <td> 0.257</td> <td>   -0.065</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>   -0.0009</td> <td>    0.000</td> <td>   -2.321</td> <td> 0.020</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>   -0.0235</td> <td>    0.001</td> <td>  -15.882</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>    0.0118</td> <td>    0.003</td> <td>    4.291</td> <td> 0.000</td> <td>    0.006</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>   -0.1752</td> <td>    0.017</td> <td>  -10.377</td> <td> 0.000</td> <td>   -0.208</td> <td>   -0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>   -0.0578</td> <td>    0.012</td> <td>   -4.701</td> <td> 0.000</td> <td>   -0.082</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>   -0.0459</td> <td>    0.023</td> <td>   -2.006</td> <td> 0.045</td> <td>   -0.091</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>    0.0584</td> <td>    0.009</td> <td>    6.398</td> <td> 0.000</td> <td>    0.041</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>    0.0005</td> <td> 5.31e-05</td> <td>    9.239</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>-1.171e-06</td> <td> 6.09e-08</td> <td>  -19.234</td> <td> 0.000</td> <td>-1.29e-06</td> <td>-1.05e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TotalSegments</th>    <td>    0.0003</td> <td> 1.54e-05</td> <td>   19.663</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TotalSections</th>    <td>   -0.0032</td> <td>    0.001</td> <td>   -4.318</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch1</th>           <td>   -0.1941</td> <td>    0.013</td> <td>  -14.890</td> <td> 0.000</td> <td>   -0.220</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch2</th>           <td>   -0.2771</td> <td>    0.016</td> <td>  -16.888</td> <td> 0.000</td> <td>   -0.309</td> <td>   -0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch3</th>           <td>    0.3724</td> <td>    0.023</td> <td>   16.227</td> <td> 0.000</td> <td>    0.327</td> <td>    0.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch4</th>           <td>   -0.0449</td> <td>    0.024</td> <td>   -1.893</td> <td> 0.058</td> <td>   -0.091</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch5</th>           <td>   -0.2675</td> <td>    0.023</td> <td>  -11.666</td> <td> 0.000</td> <td>   -0.312</td> <td>   -0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch6</th>           <td>    0.3597</td> <td>    0.019</td> <td>   19.191</td> <td> 0.000</td> <td>    0.323</td> <td>    0.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch8</th>           <td>   -0.0950</td> <td>    0.020</td> <td>   -4.790</td> <td> 0.000</td> <td>   -0.134</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch9</th>           <td>    0.4522</td> <td>    0.022</td> <td>   20.389</td> <td> 0.000</td> <td>    0.409</td> <td>    0.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch10</th>          <td>   -0.0855</td> <td>    0.024</td> <td>   -3.576</td> <td> 0.000</td> <td>   -0.132</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch11</th>          <td>   -0.0243</td> <td>    0.024</td> <td>   -1.015</td> <td> 0.310</td> <td>   -0.071</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pitch12</th>          <td>    0.5967</td> <td>    0.018</td> <td>   32.459</td> <td> 0.000</td> <td>    0.561</td> <td>    0.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre1</th>          <td>    0.0302</td> <td>    0.001</td> <td>   21.154</td> <td> 0.000</td> <td>    0.027</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre2</th>          <td>    0.0002</td> <td> 8.36e-05</td> <td>    2.866</td> <td> 0.004</td> <td> 7.57e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre3</th>          <td>   -0.0011</td> <td>    0.000</td> <td>   -9.060</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre4</th>          <td>    0.0042</td> <td>    0.000</td> <td>   23.389</td> <td> 0.000</td> <td>    0.004</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre5</th>          <td>    0.0010</td> <td>    0.000</td> <td>    8.455</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre6</th>          <td>    0.0046</td> <td>    0.000</td> <td>   16.484</td> <td> 0.000</td> <td>    0.004</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre7</th>          <td>   -0.0030</td> <td>    0.000</td> <td>  -15.256</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre8</th>          <td>    0.0010</td> <td>    0.000</td> <td>    4.069</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre9</th>          <td>   -0.0004</td> <td>    0.000</td> <td>   -1.945</td> <td> 0.052</td> <td>   -0.001</td> <td> 3.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre10</th>         <td>    0.0015</td> <td>    0.000</td> <td>    4.301</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre11</th>         <td>    0.0094</td> <td>    0.001</td> <td>   16.090</td> <td> 0.000</td> <td>    0.008</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Timbre12</th>         <td>   -0.0008</td> <td>    0.000</td> <td>   -2.451</td> <td> 0.014</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>24.146</td> <th>  Durbin-Watson:     </th> <td>   1.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  24.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.102</td> <th>  Prob(JB):          </th> <td>5.44e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.001</td> <th>  Cond. No.          </th> <td>1.27e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.27e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                valence   R-squared:                       0.484\n",
       "Model:                            OLS   Adj. R-squared:                  0.483\n",
       "Method:                 Least Squares   F-statistic:                     360.7\n",
       "Date:                Thu, 24 Feb 2022   Prob (F-statistic):               0.00\n",
       "Time:                        22:10:38   Log-Likelihood:                 6263.8\n",
       "No. Observations:               13880   AIC:                        -1.245e+04\n",
       "Df Residuals:                   13843   BIC:                        -1.217e+04\n",
       "Df Model:                          36                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "intercept           -1.2210      0.077    -15.791      0.000      -1.373      -1.069\n",
       "danceability         0.2055      0.016     12.755      0.000       0.174       0.237\n",
       "energy              -0.0239      0.021     -1.133      0.257      -0.065       0.017\n",
       "key                 -0.0009      0.000     -2.321      0.020      -0.002      -0.000\n",
       "loudness            -0.0235      0.001    -15.882      0.000      -0.026      -0.021\n",
       "mode                 0.0118      0.003      4.291      0.000       0.006       0.017\n",
       "speechiness         -0.1752      0.017    -10.377      0.000      -0.208      -0.142\n",
       "acousticness        -0.0578      0.012     -4.701      0.000      -0.082      -0.034\n",
       "instrumentalness    -0.0459      0.023     -2.006      0.045      -0.091      -0.001\n",
       "liveness             0.0584      0.009      6.398      0.000       0.041       0.076\n",
       "tempo                0.0005   5.31e-05      9.239      0.000       0.000       0.001\n",
       "duration_ms      -1.171e-06   6.09e-08    -19.234      0.000   -1.29e-06   -1.05e-06\n",
       "TotalSegments        0.0003   1.54e-05     19.663      0.000       0.000       0.000\n",
       "TotalSections       -0.0032      0.001     -4.318      0.000      -0.005      -0.002\n",
       "Pitch1              -0.1941      0.013    -14.890      0.000      -0.220      -0.169\n",
       "Pitch2              -0.2771      0.016    -16.888      0.000      -0.309      -0.245\n",
       "Pitch3               0.3724      0.023     16.227      0.000       0.327       0.417\n",
       "Pitch4              -0.0449      0.024     -1.893      0.058      -0.091       0.002\n",
       "Pitch5              -0.2675      0.023    -11.666      0.000      -0.312      -0.223\n",
       "Pitch6               0.3597      0.019     19.191      0.000       0.323       0.396\n",
       "Pitch8              -0.0950      0.020     -4.790      0.000      -0.134      -0.056\n",
       "Pitch9               0.4522      0.022     20.389      0.000       0.409       0.496\n",
       "Pitch10             -0.0855      0.024     -3.576      0.000      -0.132      -0.039\n",
       "Pitch11             -0.0243      0.024     -1.015      0.310      -0.071       0.023\n",
       "Pitch12              0.5967      0.018     32.459      0.000       0.561       0.633\n",
       "Timbre1              0.0302      0.001     21.154      0.000       0.027       0.033\n",
       "Timbre2              0.0002   8.36e-05      2.866      0.004    7.57e-05       0.000\n",
       "Timbre3             -0.0011      0.000     -9.060      0.000      -0.001      -0.001\n",
       "Timbre4              0.0042      0.000     23.389      0.000       0.004       0.005\n",
       "Timbre5              0.0010      0.000      8.455      0.000       0.001       0.001\n",
       "Timbre6              0.0046      0.000     16.484      0.000       0.004       0.005\n",
       "Timbre7             -0.0030      0.000    -15.256      0.000      -0.003      -0.003\n",
       "Timbre8              0.0010      0.000      4.069      0.000       0.001       0.002\n",
       "Timbre9             -0.0004      0.000     -1.945      0.052      -0.001    3.03e-06\n",
       "Timbre10             0.0015      0.000      4.301      0.000       0.001       0.002\n",
       "Timbre11             0.0094      0.001     16.090      0.000       0.008       0.011\n",
       "Timbre12            -0.0008      0.000     -2.451      0.014      -0.001      -0.000\n",
       "==============================================================================\n",
       "Omnibus:                       24.146   Durbin-Watson:                   1.897\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               24.243\n",
       "Skew:                          -0.102   Prob(JB):                     5.44e-06\n",
       "Kurtosis:                       3.001   Cond. No.                     1.27e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.27e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7299fa",
   "metadata": {},
   "source": [
    "#### Regression Results \n",
    "From what we can see we get pretty mediocre results from regression with an optimal R<sup>2</sup> 0.483 meaning about half of valence's value can be explained by the  37 undependent variables .\n",
    "**We keep the best subset of columns from regression in RegressionBest**\n",
    "\n",
    "$$ y = -1.2210 + 0.20 danceability\t + .... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faad5f6",
   "metadata": {},
   "source": [
    "### Q2: Predict Valence\n",
    "Now that we have prepared our data , and chosen which features we will use we continue to classification.\n",
    "We are going to keep our results , in a df for comparison in a df called TrainingResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9715d",
   "metadata": {},
   "source": [
    "#### Splitting 20% Train to 80% Test\n",
    "We split our data  80-20 Test to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11cc3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# filter method columns \n",
    "#  Regression best has the features we selected above , with pop index 0 to remove the intercept\n",
    "RegressionBest.pop(0)\n",
    "# Training Results df \n",
    "\n",
    "TrainingResults = pd.DataFrame(columns = ['Model' ,'MAE','Cross-Validation','Test Accuracy'])\n",
    "\n",
    "X = Tracksdf[RegressionBest]\n",
    "# X = Tracksdf.drop('valence', 1)\n",
    "y = Tracksdf.valence\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e09cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>TotalSegments</th>\n",
       "      <th>TotalSections</th>\n",
       "      <th>Pitch1</th>\n",
       "      <th>Pitch2</th>\n",
       "      <th>Pitch3</th>\n",
       "      <th>Pitch4</th>\n",
       "      <th>Pitch5</th>\n",
       "      <th>Pitch6</th>\n",
       "      <th>Pitch8</th>\n",
       "      <th>Pitch9</th>\n",
       "      <th>Pitch10</th>\n",
       "      <th>Pitch11</th>\n",
       "      <th>Pitch12</th>\n",
       "      <th>Timbre1</th>\n",
       "      <th>Timbre2</th>\n",
       "      <th>Timbre3</th>\n",
       "      <th>Timbre4</th>\n",
       "      <th>Timbre5</th>\n",
       "      <th>Timbre6</th>\n",
       "      <th>Timbre7</th>\n",
       "      <th>Timbre8</th>\n",
       "      <th>Timbre9</th>\n",
       "      <th>Timbre10</th>\n",
       "      <th>Timbre11</th>\n",
       "      <th>Timbre12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5dKesZwp6deuhEeW8F1UEi</th>\n",
       "      <td>0.819</td>\n",
       "      <td>0.684</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>170.187</td>\n",
       "      <td>137125</td>\n",
       "      <td>608</td>\n",
       "      <td>6</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.479324</td>\n",
       "      <td>0.211562</td>\n",
       "      <td>0.233002</td>\n",
       "      <td>0.372056</td>\n",
       "      <td>0.281424</td>\n",
       "      <td>0.332217</td>\n",
       "      <td>0.448535</td>\n",
       "      <td>0.326405</td>\n",
       "      <td>0.235053</td>\n",
       "      <td>0.267564</td>\n",
       "      <td>47.572656</td>\n",
       "      <td>39.127446</td>\n",
       "      <td>-8.497780</td>\n",
       "      <td>23.451016</td>\n",
       "      <td>32.234217</td>\n",
       "      <td>-6.573082</td>\n",
       "      <td>-10.450109</td>\n",
       "      <td>2.793360</td>\n",
       "      <td>-14.402803</td>\n",
       "      <td>9.763799</td>\n",
       "      <td>-26.312648</td>\n",
       "      <td>-5.874747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7ckGvAoSg8oq85I8owLVSZ</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.548</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.155</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>150.164</td>\n",
       "      <td>194876</td>\n",
       "      <td>669</td>\n",
       "      <td>7</td>\n",
       "      <td>0.279999</td>\n",
       "      <td>0.452765</td>\n",
       "      <td>0.335720</td>\n",
       "      <td>0.240051</td>\n",
       "      <td>0.394822</td>\n",
       "      <td>0.274552</td>\n",
       "      <td>0.230762</td>\n",
       "      <td>0.299982</td>\n",
       "      <td>0.446187</td>\n",
       "      <td>0.224752</td>\n",
       "      <td>0.322822</td>\n",
       "      <td>47.449619</td>\n",
       "      <td>-7.032794</td>\n",
       "      <td>-1.890398</td>\n",
       "      <td>-6.813454</td>\n",
       "      <td>21.948764</td>\n",
       "      <td>-11.804444</td>\n",
       "      <td>11.651496</td>\n",
       "      <td>1.400517</td>\n",
       "      <td>-10.549115</td>\n",
       "      <td>-2.421640</td>\n",
       "      <td>-14.084323</td>\n",
       "      <td>7.220040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49pu85144Xir6PgWjeyJhG</th>\n",
       "      <td>0.681</td>\n",
       "      <td>0.760</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5560</td>\n",
       "      <td>117.828</td>\n",
       "      <td>161375</td>\n",
       "      <td>431</td>\n",
       "      <td>3</td>\n",
       "      <td>0.405265</td>\n",
       "      <td>0.404007</td>\n",
       "      <td>0.483543</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.383680</td>\n",
       "      <td>0.461130</td>\n",
       "      <td>0.338587</td>\n",
       "      <td>0.326274</td>\n",
       "      <td>0.521949</td>\n",
       "      <td>0.431353</td>\n",
       "      <td>0.335671</td>\n",
       "      <td>50.858111</td>\n",
       "      <td>5.618745</td>\n",
       "      <td>10.757012</td>\n",
       "      <td>-9.661483</td>\n",
       "      <td>24.112682</td>\n",
       "      <td>-13.917766</td>\n",
       "      <td>-8.845582</td>\n",
       "      <td>3.745761</td>\n",
       "      <td>-14.350958</td>\n",
       "      <td>7.008571</td>\n",
       "      <td>-10.536343</td>\n",
       "      <td>-4.421543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1YqcGlCHNquxBhlUZsjhMT</th>\n",
       "      <td>0.728</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0589</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>171.083</td>\n",
       "      <td>190328</td>\n",
       "      <td>757</td>\n",
       "      <td>7</td>\n",
       "      <td>0.440683</td>\n",
       "      <td>0.569787</td>\n",
       "      <td>0.282478</td>\n",
       "      <td>0.417465</td>\n",
       "      <td>0.265410</td>\n",
       "      <td>0.325745</td>\n",
       "      <td>0.256408</td>\n",
       "      <td>0.322283</td>\n",
       "      <td>0.210679</td>\n",
       "      <td>0.330550</td>\n",
       "      <td>0.182267</td>\n",
       "      <td>50.568159</td>\n",
       "      <td>32.094684</td>\n",
       "      <td>-10.957024</td>\n",
       "      <td>22.730683</td>\n",
       "      <td>27.941406</td>\n",
       "      <td>-20.330421</td>\n",
       "      <td>-9.443041</td>\n",
       "      <td>2.008333</td>\n",
       "      <td>-0.501440</td>\n",
       "      <td>7.943684</td>\n",
       "      <td>-20.645995</td>\n",
       "      <td>-3.748717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4S1GTwPVpkSy7CsJIDKPEz</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.560</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>98.940</td>\n",
       "      <td>207773</td>\n",
       "      <td>662</td>\n",
       "      <td>6</td>\n",
       "      <td>0.440654</td>\n",
       "      <td>0.601828</td>\n",
       "      <td>0.386648</td>\n",
       "      <td>0.330524</td>\n",
       "      <td>0.407062</td>\n",
       "      <td>0.364840</td>\n",
       "      <td>0.332414</td>\n",
       "      <td>0.373524</td>\n",
       "      <td>0.375884</td>\n",
       "      <td>0.291991</td>\n",
       "      <td>0.305210</td>\n",
       "      <td>46.525921</td>\n",
       "      <td>51.118465</td>\n",
       "      <td>-20.940952</td>\n",
       "      <td>-11.326582</td>\n",
       "      <td>16.091089</td>\n",
       "      <td>-4.263154</td>\n",
       "      <td>-0.488270</td>\n",
       "      <td>4.477520</td>\n",
       "      <td>-17.426390</td>\n",
       "      <td>5.217249</td>\n",
       "      <td>-15.506453</td>\n",
       "      <td>0.403793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5rPhugY1JSJCJWIRiniDta</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.542</td>\n",
       "      <td>8</td>\n",
       "      <td>-8.627</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>190.724</td>\n",
       "      <td>255434</td>\n",
       "      <td>882</td>\n",
       "      <td>9</td>\n",
       "      <td>0.337724</td>\n",
       "      <td>0.566702</td>\n",
       "      <td>0.276365</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.337154</td>\n",
       "      <td>0.265091</td>\n",
       "      <td>0.260926</td>\n",
       "      <td>0.395365</td>\n",
       "      <td>0.229007</td>\n",
       "      <td>0.264058</td>\n",
       "      <td>0.272246</td>\n",
       "      <td>45.975931</td>\n",
       "      <td>25.771549</td>\n",
       "      <td>-16.330603</td>\n",
       "      <td>-22.591001</td>\n",
       "      <td>45.943338</td>\n",
       "      <td>-18.697349</td>\n",
       "      <td>-19.545287</td>\n",
       "      <td>1.760719</td>\n",
       "      <td>-8.270508</td>\n",
       "      <td>2.942875</td>\n",
       "      <td>-12.146753</td>\n",
       "      <td>-5.231768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0AICBlLzFCTpUqmAbtzB2z</th>\n",
       "      <td>0.659</td>\n",
       "      <td>0.614</td>\n",
       "      <td>9</td>\n",
       "      <td>-7.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>119.405</td>\n",
       "      <td>143969</td>\n",
       "      <td>409</td>\n",
       "      <td>5</td>\n",
       "      <td>0.475998</td>\n",
       "      <td>0.375565</td>\n",
       "      <td>0.319562</td>\n",
       "      <td>0.198279</td>\n",
       "      <td>0.424462</td>\n",
       "      <td>0.383254</td>\n",
       "      <td>0.321824</td>\n",
       "      <td>0.268061</td>\n",
       "      <td>0.447667</td>\n",
       "      <td>0.251663</td>\n",
       "      <td>0.331582</td>\n",
       "      <td>47.038685</td>\n",
       "      <td>69.706389</td>\n",
       "      <td>0.856702</td>\n",
       "      <td>-9.419765</td>\n",
       "      <td>29.880643</td>\n",
       "      <td>-17.802983</td>\n",
       "      <td>1.058616</td>\n",
       "      <td>-3.833555</td>\n",
       "      <td>-8.554081</td>\n",
       "      <td>0.236068</td>\n",
       "      <td>-13.574917</td>\n",
       "      <td>-19.603985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5Qht2aUJcCjRuhrlHvvKt2</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.858</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.901</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>132.003</td>\n",
       "      <td>223672</td>\n",
       "      <td>666</td>\n",
       "      <td>10</td>\n",
       "      <td>0.491623</td>\n",
       "      <td>0.459281</td>\n",
       "      <td>0.494958</td>\n",
       "      <td>0.381830</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.389129</td>\n",
       "      <td>0.409992</td>\n",
       "      <td>0.361539</td>\n",
       "      <td>0.379297</td>\n",
       "      <td>0.496808</td>\n",
       "      <td>0.384989</td>\n",
       "      <td>53.872590</td>\n",
       "      <td>76.765273</td>\n",
       "      <td>13.539059</td>\n",
       "      <td>-9.165393</td>\n",
       "      <td>38.401458</td>\n",
       "      <td>-12.490527</td>\n",
       "      <td>0.155799</td>\n",
       "      <td>5.869468</td>\n",
       "      <td>-24.564018</td>\n",
       "      <td>6.087847</td>\n",
       "      <td>-12.400622</td>\n",
       "      <td>-2.041482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3IkACj2ZEsmDLzhQNBW2XH</th>\n",
       "      <td>0.656</td>\n",
       "      <td>0.871</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.604</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8460</td>\n",
       "      <td>140.322</td>\n",
       "      <td>320920</td>\n",
       "      <td>1458</td>\n",
       "      <td>16</td>\n",
       "      <td>0.535956</td>\n",
       "      <td>0.535323</td>\n",
       "      <td>0.424075</td>\n",
       "      <td>0.413586</td>\n",
       "      <td>0.461443</td>\n",
       "      <td>0.550259</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>0.404298</td>\n",
       "      <td>0.374760</td>\n",
       "      <td>0.355558</td>\n",
       "      <td>0.409509</td>\n",
       "      <td>49.524754</td>\n",
       "      <td>37.934514</td>\n",
       "      <td>7.389999</td>\n",
       "      <td>7.987697</td>\n",
       "      <td>27.217972</td>\n",
       "      <td>6.693215</td>\n",
       "      <td>-21.961180</td>\n",
       "      <td>2.399360</td>\n",
       "      <td>-9.624700</td>\n",
       "      <td>11.316265</td>\n",
       "      <td>-18.385636</td>\n",
       "      <td>-2.679483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5Q7gmVXIdNT2s840PYzYNU</th>\n",
       "      <td>0.716</td>\n",
       "      <td>0.743</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.696</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>84.046</td>\n",
       "      <td>180199</td>\n",
       "      <td>714</td>\n",
       "      <td>5</td>\n",
       "      <td>0.514828</td>\n",
       "      <td>0.517193</td>\n",
       "      <td>0.331905</td>\n",
       "      <td>0.222119</td>\n",
       "      <td>0.337342</td>\n",
       "      <td>0.275779</td>\n",
       "      <td>0.283674</td>\n",
       "      <td>0.341001</td>\n",
       "      <td>0.521102</td>\n",
       "      <td>0.410542</td>\n",
       "      <td>0.303349</td>\n",
       "      <td>48.277326</td>\n",
       "      <td>49.467195</td>\n",
       "      <td>8.587126</td>\n",
       "      <td>-1.063179</td>\n",
       "      <td>19.311920</td>\n",
       "      <td>-1.151538</td>\n",
       "      <td>4.065290</td>\n",
       "      <td>7.744209</td>\n",
       "      <td>-21.296969</td>\n",
       "      <td>1.822364</td>\n",
       "      <td>-21.536465</td>\n",
       "      <td>-5.324849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11104 rows  36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        danceability  energy  key  loudness  mode  \\\n",
       "song_id                                                             \n",
       "5dKesZwp6deuhEeW8F1UEi         0.819   0.684    6    -7.169     0   \n",
       "7ckGvAoSg8oq85I8owLVSZ         0.590   0.548    6    -7.155     0   \n",
       "49pu85144Xir6PgWjeyJhG         0.681   0.760    2    -5.252     0   \n",
       "1YqcGlCHNquxBhlUZsjhMT         0.728   0.810    1    -5.200     1   \n",
       "4S1GTwPVpkSy7CsJIDKPEz         0.516   0.560    6    -7.116     0   \n",
       "...                              ...     ...  ...       ...   ...   \n",
       "5rPhugY1JSJCJWIRiniDta         0.390   0.542    8    -8.627     0   \n",
       "0AICBlLzFCTpUqmAbtzB2z         0.659   0.614    9    -7.320     0   \n",
       "5Qht2aUJcCjRuhrlHvvKt2         0.752   0.858   10    -2.901     1   \n",
       "3IkACj2ZEsmDLzhQNBW2XH         0.656   0.871    1    -4.604     1   \n",
       "5Q7gmVXIdNT2s840PYzYNU         0.716   0.743    2    -5.696     1   \n",
       "\n",
       "                        speechiness  acousticness  instrumentalness  liveness  \\\n",
       "song_id                                                                         \n",
       "5dKesZwp6deuhEeW8F1UEi       0.1190      0.132000          0.000000    0.0942   \n",
       "7ckGvAoSg8oq85I8owLVSZ       0.2670      0.425000          0.000216    0.1490   \n",
       "49pu85144Xir6PgWjeyJhG       0.1400      0.638000          0.000000    0.5560   \n",
       "1YqcGlCHNquxBhlUZsjhMT       0.0589      0.224000          0.000011    0.0952   \n",
       "4S1GTwPVpkSy7CsJIDKPEz       0.4860      0.032200          0.000000    0.1500   \n",
       "...                             ...           ...               ...       ...   \n",
       "5rPhugY1JSJCJWIRiniDta       0.4770      0.411000          0.000000    0.1040   \n",
       "0AICBlLzFCTpUqmAbtzB2z       0.0541      0.000404          0.000000    0.1230   \n",
       "5Qht2aUJcCjRuhrlHvvKt2       0.1260      0.037800          0.000000    0.0541   \n",
       "3IkACj2ZEsmDLzhQNBW2XH       0.3660      0.573000          0.000000    0.8460   \n",
       "5Q7gmVXIdNT2s840PYzYNU       0.3580      0.061500          0.000000    0.5110   \n",
       "\n",
       "                          tempo  duration_ms  TotalSegments  TotalSections  \\\n",
       "song_id                                                                      \n",
       "5dKesZwp6deuhEeW8F1UEi  170.187       137125            608              6   \n",
       "7ckGvAoSg8oq85I8owLVSZ  150.164       194876            669              7   \n",
       "49pu85144Xir6PgWjeyJhG  117.828       161375            431              3   \n",
       "1YqcGlCHNquxBhlUZsjhMT  171.083       190328            757              7   \n",
       "4S1GTwPVpkSy7CsJIDKPEz   98.940       207773            662              6   \n",
       "...                         ...          ...            ...            ...   \n",
       "5rPhugY1JSJCJWIRiniDta  190.724       255434            882              9   \n",
       "0AICBlLzFCTpUqmAbtzB2z  119.405       143969            409              5   \n",
       "5Qht2aUJcCjRuhrlHvvKt2  132.003       223672            666             10   \n",
       "3IkACj2ZEsmDLzhQNBW2XH  140.322       320920           1458             16   \n",
       "5Q7gmVXIdNT2s840PYzYNU   84.046       180199            714              5   \n",
       "\n",
       "                          Pitch1    Pitch2    Pitch3    Pitch4    Pitch5  \\\n",
       "song_id                                                                    \n",
       "5dKesZwp6deuhEeW8F1UEi  0.329344  0.479324  0.211562  0.233002  0.372056   \n",
       "7ckGvAoSg8oq85I8owLVSZ  0.279999  0.452765  0.335720  0.240051  0.394822   \n",
       "49pu85144Xir6PgWjeyJhG  0.405265  0.404007  0.483543  0.296100  0.383680   \n",
       "1YqcGlCHNquxBhlUZsjhMT  0.440683  0.569787  0.282478  0.417465  0.265410   \n",
       "4S1GTwPVpkSy7CsJIDKPEz  0.440654  0.601828  0.386648  0.330524  0.407062   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "5rPhugY1JSJCJWIRiniDta  0.337724  0.566702  0.276365  0.438155  0.337154   \n",
       "0AICBlLzFCTpUqmAbtzB2z  0.475998  0.375565  0.319562  0.198279  0.424462   \n",
       "5Qht2aUJcCjRuhrlHvvKt2  0.491623  0.459281  0.494958  0.381830  0.288026   \n",
       "3IkACj2ZEsmDLzhQNBW2XH  0.535956  0.535323  0.424075  0.413586  0.461443   \n",
       "5Q7gmVXIdNT2s840PYzYNU  0.514828  0.517193  0.331905  0.222119  0.337342   \n",
       "\n",
       "                          Pitch6    Pitch8    Pitch9   Pitch10   Pitch11  \\\n",
       "song_id                                                                    \n",
       "5dKesZwp6deuhEeW8F1UEi  0.281424  0.332217  0.448535  0.326405  0.235053   \n",
       "7ckGvAoSg8oq85I8owLVSZ  0.274552  0.230762  0.299982  0.446187  0.224752   \n",
       "49pu85144Xir6PgWjeyJhG  0.461130  0.338587  0.326274  0.521949  0.431353   \n",
       "1YqcGlCHNquxBhlUZsjhMT  0.325745  0.256408  0.322283  0.210679  0.330550   \n",
       "4S1GTwPVpkSy7CsJIDKPEz  0.364840  0.332414  0.373524  0.375884  0.291991   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "5rPhugY1JSJCJWIRiniDta  0.265091  0.260926  0.395365  0.229007  0.264058   \n",
       "0AICBlLzFCTpUqmAbtzB2z  0.383254  0.321824  0.268061  0.447667  0.251663   \n",
       "5Qht2aUJcCjRuhrlHvvKt2  0.389129  0.409992  0.361539  0.379297  0.496808   \n",
       "3IkACj2ZEsmDLzhQNBW2XH  0.550259  0.460005  0.404298  0.374760  0.355558   \n",
       "5Q7gmVXIdNT2s840PYzYNU  0.275779  0.283674  0.341001  0.521102  0.410542   \n",
       "\n",
       "                         Pitch12    Timbre1    Timbre2    Timbre3    Timbre4  \\\n",
       "song_id                                                                        \n",
       "5dKesZwp6deuhEeW8F1UEi  0.267564  47.572656  39.127446  -8.497780  23.451016   \n",
       "7ckGvAoSg8oq85I8owLVSZ  0.322822  47.449619  -7.032794  -1.890398  -6.813454   \n",
       "49pu85144Xir6PgWjeyJhG  0.335671  50.858111   5.618745  10.757012  -9.661483   \n",
       "1YqcGlCHNquxBhlUZsjhMT  0.182267  50.568159  32.094684 -10.957024  22.730683   \n",
       "4S1GTwPVpkSy7CsJIDKPEz  0.305210  46.525921  51.118465 -20.940952 -11.326582   \n",
       "...                          ...        ...        ...        ...        ...   \n",
       "5rPhugY1JSJCJWIRiniDta  0.272246  45.975931  25.771549 -16.330603 -22.591001   \n",
       "0AICBlLzFCTpUqmAbtzB2z  0.331582  47.038685  69.706389   0.856702  -9.419765   \n",
       "5Qht2aUJcCjRuhrlHvvKt2  0.384989  53.872590  76.765273  13.539059  -9.165393   \n",
       "3IkACj2ZEsmDLzhQNBW2XH  0.409509  49.524754  37.934514   7.389999   7.987697   \n",
       "5Q7gmVXIdNT2s840PYzYNU  0.303349  48.277326  49.467195   8.587126  -1.063179   \n",
       "\n",
       "                          Timbre5    Timbre6    Timbre7   Timbre8    Timbre9  \\\n",
       "song_id                                                                        \n",
       "5dKesZwp6deuhEeW8F1UEi  32.234217  -6.573082 -10.450109  2.793360 -14.402803   \n",
       "7ckGvAoSg8oq85I8owLVSZ  21.948764 -11.804444  11.651496  1.400517 -10.549115   \n",
       "49pu85144Xir6PgWjeyJhG  24.112682 -13.917766  -8.845582  3.745761 -14.350958   \n",
       "1YqcGlCHNquxBhlUZsjhMT  27.941406 -20.330421  -9.443041  2.008333  -0.501440   \n",
       "4S1GTwPVpkSy7CsJIDKPEz  16.091089  -4.263154  -0.488270  4.477520 -17.426390   \n",
       "...                           ...        ...        ...       ...        ...   \n",
       "5rPhugY1JSJCJWIRiniDta  45.943338 -18.697349 -19.545287  1.760719  -8.270508   \n",
       "0AICBlLzFCTpUqmAbtzB2z  29.880643 -17.802983   1.058616 -3.833555  -8.554081   \n",
       "5Qht2aUJcCjRuhrlHvvKt2  38.401458 -12.490527   0.155799  5.869468 -24.564018   \n",
       "3IkACj2ZEsmDLzhQNBW2XH  27.217972   6.693215 -21.961180  2.399360  -9.624700   \n",
       "5Q7gmVXIdNT2s840PYzYNU  19.311920  -1.151538   4.065290  7.744209 -21.296969   \n",
       "\n",
       "                         Timbre10   Timbre11   Timbre12  \n",
       "song_id                                                  \n",
       "5dKesZwp6deuhEeW8F1UEi   9.763799 -26.312648  -5.874747  \n",
       "7ckGvAoSg8oq85I8owLVSZ  -2.421640 -14.084323   7.220040  \n",
       "49pu85144Xir6PgWjeyJhG   7.008571 -10.536343  -4.421543  \n",
       "1YqcGlCHNquxBhlUZsjhMT   7.943684 -20.645995  -3.748717  \n",
       "4S1GTwPVpkSy7CsJIDKPEz   5.217249 -15.506453   0.403793  \n",
       "...                           ...        ...        ...  \n",
       "5rPhugY1JSJCJWIRiniDta   2.942875 -12.146753  -5.231768  \n",
       "0AICBlLzFCTpUqmAbtzB2z   0.236068 -13.574917 -19.603985  \n",
       "5Qht2aUJcCjRuhrlHvvKt2   6.087847 -12.400622  -2.041482  \n",
       "3IkACj2ZEsmDLzhQNBW2XH  11.316265 -18.385636  -2.679483  \n",
       "5Q7gmVXIdNT2s840PYzYNU   1.822364 -21.536465  -5.324849  \n",
       "\n",
       "[11104 rows x 36 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714796d7",
   "metadata": {},
   "source": [
    "#### Scaling and Normalization \n",
    "\n",
    "Standar Scaler Min-Max Scaler and Normalization were used but led to worse results . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59726eb3",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaac525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3329e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.1439\n",
      "Cross validation 0.1492\n",
      "Training accuracy 0.4264\n",
      "Testing accuracy 0.3097\n"
     ]
    }
   ],
   "source": [
    "simple_tree = DecisionTreeRegressor(max_depth=8, max_features='auto', max_leaf_nodes=90,\n",
    "                      min_samples_leaf=10)\n",
    "simple_tree.fit(X_train, y_train)\n",
    "predicted = simple_tree.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores = cross_val_score(simple_tree, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(predicted, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(simple_tree.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(simple_tree.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(simple_tree.score(X_test, y_test)))\n",
    "\n",
    "dict = {'Model': 'Decision Tree', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ef0c5",
   "metadata": {},
   "source": [
    "#### Decision Tree Hyper Parameteres Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f5abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = [\n",
    "#   {'max_depth': list(range(1, 11)),\n",
    "#     \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n",
    "#     \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "#     \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90] }]\n",
    " \n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "# clf = GridSearchCV(DecisionTreeRegressor(), parameters, cv=cv , scoring =\"neg_mean_absolute_error\" )\n",
    "\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# print(clf.best_estimator_)\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885a2a8",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10a7c2fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8008/406000558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    443\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iliad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(  random_state = 42)\n",
    "\n",
    "rf.fit(X_train, y_train);\n",
    "predicted = rf.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(rf, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(predicted, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(rf.score(X_test, y_test))\n",
    "print(\"MAE\",MAE)\n",
    "\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(rf.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(rf.score(X_test, y_test)))\n",
    "\n",
    "dict = {'Model': 'Random Forest', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d916b3",
   "metadata": {},
   "source": [
    "#### Random Forest hyper tuning\n",
    "Here we tried an alternative to the brute force grid search , we created a rather large search space, and implemented\n",
    "randomized search , this may not return a better results , but it allows to stretch the search space [Random Forest Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). \n",
    "\n",
    "* n_estimators = number of trees in the foreset\n",
    "* max_features = max number of features considered for splitting a node\n",
    "* max_depth = max number of levels in each decision tree\n",
    "* min_samples_split = min number of data points placed in a node before the node is split\n",
    "* min_samples_leaf = min number of data points allowed in a leaf node\n",
    "* bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a21148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'n_estimators': [50,100,200,],\n",
    "#                'max_features':['auto', 'sqrt'],\n",
    "#                'max_depth': [10, 20 ,50, 60, 90, 100, None],\n",
    "#                'min_samples_split': [2, 6,8, 10],\n",
    "#                'min_samples_leaf': [1, 3, 4 ,6],\n",
    "#                'bootstrap': [True, False]\n",
    "#          }\n",
    "# clf = RandomizedSearchCV(rf, params, cv=cv , scoring =\"neg_mean_absolute_error\" )\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# print(clf.best_estimator_)\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f13c1",
   "metadata": {},
   "source": [
    "Like i mentioned above , Randomized Search doesn't return the optimal results because it doesn't try all the combinations , \n",
    "unfortunately the 'optimal'  hyper parameteres we got under-performed the defaults ones . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a3ad0",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LinearRegressor = LinearRegression(fit_intercept='False')\n",
    "LinearRegressor.fit(X_train, y_train)\n",
    "predictions = LinearRegressor.predict(X_test)\n",
    "\n",
    "\n",
    "scores = cross_val_score(LinearRegressor, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(predictions, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(LinearRegressor.score(X_test, y_test))\n",
    "print(\"MAE\",MAE)\n",
    "\n",
    "\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(LinearRegressor.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(LinearRegressor.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "dict = {'Model': 'Linear Regression', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273d517",
   "metadata": {},
   "source": [
    "Linear regression doesn't really have hyper parameteres to tune .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8ea22",
   "metadata": {},
   "source": [
    "#### XGB Boost Regressor \n",
    "The documentation for the parameters can be found here  [Documentation](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-linear-booster-booster-gblinear)\n",
    "The objective was the only thing that was tested by hand . \n",
    "* Note beside the objective and number of boosting rouns , the grid search returned the default parameters as the optimal , this is why we dont change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=355,objective='binary:logistic' ,verbosity= 0)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "predictions = xgb_reg.predict(X_test)\n",
    "\n",
    "\n",
    "scores =cross_val_score(xgb_reg, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(predictions, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(xgb_reg.score(X_test, y_test))\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(xgb_reg.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(xgb_reg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "\n",
    "dict = {'Model': 'XGB', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afca41d",
   "metadata": {},
   "source": [
    "#### XGB Hyper Parameters tuning \n",
    "Tuning Parameters  , [Documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) \n",
    "* max_depth\n",
    "* min child weight\n",
    "* eta\n",
    "* subsample\n",
    "* colsample_bytree \n",
    "* objective.\n",
    "\n",
    "A total number of combinations for the set of parameters above is a product of options for each parameter (3 x 5 x 3 x 3 x 3 = 405). It also needs to be multiplied by 5 to calculate a total number of data-fitting runs as we will be doing 5-fold cross-validation.\n",
    "\n",
    "But before that   we are going to tune the number of boosting rounds or trees to build\n",
    "* num boost round corresponds to the number of boosting rounds or trees to build\n",
    "* early_stopping_round corresponds , to the amount of 'rounds' we allow without some improvement. \n",
    "\n",
    "Instead of numpy arrays or pandas dataFrame, XGBoost uses DMatrices , we use the Native API  to find the optimal number of boosting rounds . Instead of pandas dfs  XGBoost uses DMatrices that contain both the features and the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31678f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# #  initial parameters - change and compare\n",
    "# params = {\n",
    "#     # Parameters that we are going to tune , we feed the default ones here to tune the boosting rounds . \n",
    "#     'max_depth':6,\n",
    "#     'min_child_weight': 1,\n",
    "#     'eta':.3,\n",
    "#     'subsample': 1,\n",
    "#     'colsample_bytree': 1,\n",
    "#     # Other parameters\n",
    "#     'objective':'reg:linear',\n",
    "# }\n",
    "# params['eval_metric'] = \"mae\"\n",
    "\n",
    "# num_boost_round = 999\n",
    "\n",
    "# model = xgb.train(\n",
    "#     params,\n",
    "#     dtrain,\n",
    "#     num_boost_round=num_boost_round,\n",
    "#     evals=[(dtest, \"Test\")],\n",
    "#     early_stopping_rounds=10\n",
    "# )\n",
    "# print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "#                  model.best_score,\n",
    "#                  model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3facad",
   "metadata": {},
   "source": [
    "This is the process to find the hyper parameters we run a grid search for 405 combinations ,it takes along time so i commented it out , the optimal hyper parameteres were pretty close to the default ones ,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdad824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'gamma': [0,0.5, 1, 1.5, 2, 5],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5,6]\n",
    "    \n",
    "#         }\n",
    "# clf = GridSearchCV(xgb_reg, params, cv=cv , scoring =\"neg_mean_absolute_error\" )\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# print(clf.best_estimator_)\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1982ca",
   "metadata": {},
   "source": [
    "#### Light GBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor(n_jobs =  8 , reg_lambda= 0.1, reg_alpha=  0, num_leaves= 80, n_estimators= 400, max_depth= -1)\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "preds = lgb_reg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores = cross_val_score(lgb_reg, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(preds, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(lgb_reg.score(X_test, y_test))\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(lgb_reg.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(lgb_reg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "dict = {'Model': 'LightGBM', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d78a3",
   "metadata": {},
   "source": [
    "#### Light GBM Tuning\n",
    "Once again we try our luck with a larger search space and a randomized search , this time with better results ,\n",
    "getting the best results we have so far with 0.64 testing accuracy and 0.115 Cross \n",
    "The hyper parameters we are going to tune , [Documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) : \n",
    "\n",
    "* num_leaves - Maximum tree leaves for base learners.\n",
    "* max_depth  - Maximum tree depth for base learners\n",
    "* reg_alpha  - L1 regularization term on weights.\n",
    "* reg_lambda - L2 regularization term on weights.\n",
    "* n_estimators - Number of boosted trees to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27846763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "# lgb_reg = lgb.LGBMRegressor(verbose=-1)\n",
    "\n",
    "\n",
    "# params ={\n",
    "#        'num_leaves' : [20, 31,60,80,100  ,200],\n",
    "#         'max_depth' : [-1,10,15 , 25 ,50 , 80  ],\n",
    "#         'reg_alpha': [0, 1e-1, 1, 2, 5, 7 ],\n",
    "#         'reg_lambda': [0, 1e-1, 1, 5, 10 ],\n",
    "#         'n_estimators' :[100,150,250,300,400,500],\n",
    "\n",
    "# }\n",
    "# clf = RandomizedSearchCV(lgb_reg, params, cv=cv , scoring =\"neg_mean_absolute_error\" )\n",
    "# clf =  clf.fit(X_train, y_train)\n",
    "# print(clf.best_estimator_)\n",
    "# print(\"Best Light GBM  Parameters\",clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17944f8e",
   "metadata": {},
   "source": [
    "#### Cat Boost regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "cb_reg = cb.CatBoostRegressor(silent = True,  depth= 10,  iterations = 1000 )\n",
    "cb_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = cb_reg.predict(X_test)\n",
    "\n",
    "\n",
    "scores = cross_val_score(cb_reg, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(preds, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(cb_reg.score(X_test, y_test))\n",
    "\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(cb_reg.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(cb_reg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "dict = {'Model': 'CatBoost', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dd39a",
   "metadata": {},
   "source": [
    "#### Cat Boost Hyper Parameters Tuning\n",
    "Parameters for Tuning , [Documentation](https://catboost.ai/en/docs/concepts/parameter-tuning) ,\n",
    "Cat Boost tuning was the hardest because the documentation does not include the default hyper parameters , so we kept getting\n",
    "worse results than the baseline after tuning .  \n",
    "Parameters we tried to tune\n",
    "* Iterations is a synonym for n_estimators\n",
    "* Depth of the tree.\n",
    "* Coefficient at the L2 regularization term of the cost function.\n",
    "* The number of splits for numerical features.\n",
    "\n",
    "We actually tuned only for depth and iterations , \n",
    "the other ones expanded the search space enormously taking more than 6 hours to return results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'depth':[3,1,2,6,4,5,7,8,9,10],\n",
    "#           'iterations':[250,100,500,1000],\n",
    "#          }\n",
    "    \n",
    "# clf = GridSearchCV(cb_reg, params, cv=cv , scoring =\"neg_mean_absolute_error\" )\n",
    "# clf =  clf.fit(X_train, y_train)\n",
    "# print(clf.best_estimator_)\n",
    "# print(\"Best Light GBM  Parameters\",clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded0b3d",
   "metadata": {},
   "source": [
    "#### Voting Ensembles -Voting Regressor beetween  XGB and Light GBM\n",
    "We use a voting regressor here combining the outputs of XGB and Light GBM  , while keeping the optimal hyper paramemeters we found for each one . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "vreg = VotingRegressor(estimators=[('xgb', xgb_reg), \n",
    "                                   ('lgb', lgb_reg)],\n",
    "                       n_jobs=8)\n",
    "\n",
    "vreg.fit(X_train, y_train)\n",
    "preds = vreg.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(vreg, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(preds, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(vreg.score(X_test, y_test))\n",
    "\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(vreg.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(vreg.score(X_test, y_test)))\n",
    "\n",
    "dict = {'Model': 'Voting Ensemble', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2c2fe",
   "metadata": {},
   "source": [
    "#### Stacked Ensembles\n",
    "Here we create a stack feeding the results of XGB and Light GBM   to a Linear Regression . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "estimators = [('xgb', xgb_reg), ('lgb',  lgb_reg)]\n",
    "final_estimator = LinearRegression()\n",
    "sreg = StackingRegressor(estimators=estimators, final_estimator=final_estimator   , n_jobs=8)\n",
    "\n",
    "sreg.fit(X_train, y_train)\n",
    "preds = sreg.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(sreg, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "\n",
    "MAE = '{:.4f}'.format(mean_absolute_error(preds, y_test))\n",
    "Cross = '{:.4f}'.format(np.mean(-scores))\n",
    "Test = '{:.4f}'.format(sreg.score(X_test, y_test))\n",
    "\n",
    "print(\"MAE\",MAE)\n",
    "print('Cross validation' , Cross)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Training accuracy {:.4f}'.format(sreg.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(sreg.score(X_test, y_test)))\n",
    "\n",
    "dict = {'Model': 'Stacked Ensembles', 'MAE': MAE, 'Cross-Validation': Cross, 'Test Accuracy': Test}\n",
    "\n",
    "TrainingResults  = TrainingResults.append(dict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dc1ec",
   "metadata": {},
   "source": [
    "### Connectivist Approaches Summary \n",
    "The best non connectivist results , came from a Stacked Ensemble of :\n",
    "* XGB with hyper-parameters  n_estimators=355,objective='binary:logistic\n",
    "* Light GBM with hyper-parameteres  reg_lambda= 0.1, reg_alpha=  0, num_leaves= 80, n_estimators= 400, max_depth= -1\n",
    "* To a final estimator of Linear Regression\n",
    "**Final Results MAE 0.0935  , Cross Validation 0.1142 , and Test Accuracy 0.6522*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e437e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TrainingResults .sort_values(by=['Test Accuracy'] , ascending=False))\n",
    "sreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b205e85",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "Like we mentioned above we have the XGB regressor , the Light GBM , and the Linear Regressor , we are going to plot for each one of them specificaly the most important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_reg, ax = fig.gca())\n",
    "fig.suptitle('XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89faf4d",
   "metadata": {},
   "source": [
    "* We can see how not scaling our features effected us in a bad way by putting the duration ms in the top even if it's not highly correlated with the valence . \n",
    "* Apart from that we see a kind of balanced feature importance with Pitch 9 , tempo pitch 6 and pitch 12 being at the top , and the key total sections and mode being almost insignificant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "lgb.plot_importance(lgb_reg, ax = fig.gca())\n",
    "fig.suptitle('Light GBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596579a",
   "metadata": {},
   "source": [
    "* We seem to have the same problem obviously in the Light GBM with the duration_ms dominating , but apart from that we can see different fetures being at the top , Pitch 1 Pitch 12 and Timbre 7 .\n",
    "* The same features at the bottom through . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f77b0",
   "metadata": {},
   "source": [
    "To calculate the feature importance for the Linear Regression Models  we are to going to multiply the standar deviation with the coefficient of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2374c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating a DF with all the coefficients for each feature\n",
    "features = pd.DataFrame(LinearRegressor.coef_, X.columns, columns=['coefficient'])\n",
    "features.coefficient = features.coefficient.abs()\n",
    "\n",
    "# Storing for each feature its standar deviation \n",
    "features[\"stdev\"] = np.array(X.std()).reshape(-1,1)\n",
    "#  multiply\n",
    "features[\"importance\"] = features[\"coefficient\"] * features[\"stdev\"]\n",
    "# sort by importance\n",
    "features =  features .sort_values(by=['importance'] , ascending=False)\n",
    "# plot\n",
    "features.plot.bar(y='importance' , figsize=(18,5) , fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d1321",
   "metadata": {},
   "source": [
    "Different results than the boosting methods ,\n",
    "* Timbre 1  , dominates the importance plot , something we see for the first time , other methods were more balanced \n",
    "* Like the other methods , we see some features that are negligeble , perharps we should drop them ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa5ae3",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7073d3",
   "metadata": {},
   "source": [
    "#### Sequential Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed9287",
   "metadata": {},
   "source": [
    "We already have the train to test split , and the transforming and cleaning of the data so we can get straight to the building of the model.\n",
    "Although the scaling of our data in the connectivist approaches did not yield impovement of our models , we are going to include a normalization layer .\n",
    "What we have below is :\n",
    "\n",
    "* The normalization layer\n",
    "* Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity and 64 nodes.\n",
    "* A linear Dense single-output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model():\n",
    "  model = keras.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c56eae",
   "metadata": {},
   "source": [
    "Let's see a summary of the model.\n",
    "\n",
    "Note that there are 73 non-trainable parameters. These are the parameters of the normalization layer.\n",
    "\n",
    "For nine features we have a mean and a standard deviation per feature plus a total count,  $2 \\times 36 + 1 = 73$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc01e88",
   "metadata": {},
   "source": [
    "**Let's start training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee555f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "     y_train,\n",
    "    epochs=num_epochs, \n",
    "    validation_split=0.2,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa543a7",
   "metadata": {},
   "source": [
    "* 0.12 Average decline from the real value to the predicted one , pretty bad . \n",
    "* We can see that the model shows little to no improvement , after the 37-38 epoch \n",
    "* We need to improve the model , to stop early after a certain amount of epochs have passed with improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3293dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs,\n",
    "                    validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fcd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430142d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e81528",
   "metadata": {},
   "source": [
    "Slight Improvement but pretty far from our optimal non-connectivist method . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c8dad",
   "metadata": {},
   "source": [
    "#### Different Model Sizes and Methods - Weight Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "def build_and_compile_MediumModel():\n",
    "  model = keras.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(64, activation='elu', input_shape=(36,)),\n",
    "      layers.Dense(64, activation='elu'),\n",
    "      layers.Dense(64, activation='elu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "\n",
    "def build_and_compile_L2Model():\n",
    "  model = keras.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(36,)),\n",
    "      layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "      layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "      layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d532a25",
   "metadata": {},
   "source": [
    "#### Medium Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_MediumModel()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs,\n",
    "                    validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351e3fa",
   "metadata": {},
   "source": [
    "**Slight improvement by adding another layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79433ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b157cfa",
   "metadata": {},
   "source": [
    "#### Wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(256, activation='relu', input_shape=(36,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092a77a",
   "metadata": {},
   "source": [
    "#### L2 Weight Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_L2Model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs,\n",
    "                    validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265baac",
   "metadata": {},
   "source": [
    "### Tuning Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e00cd",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "This concludes the assignment , The optimal MAE we got from non connectivist approaches a stack of Light XGB  to a finaly estimator of linear regression , was 0.0935 , while the best results from neural network came from 3 layers of 256 , 256 and 128 nodes with relu activation and a standar normalizer.  for a MAE of 0.099\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
